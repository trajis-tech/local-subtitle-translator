from __future__ import annotations

import asyncio
import csv
import json
import re
import shutil
import tempfile
import threading
import time
import traceback
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Optional

import gradio as gr
import pysrt

from src.config import load_config, AppConfig
from src.glossary import (
    GlossaryEntry, load_glossary, save_glossary,
    import_csv_two_cols, import_subtitleedit_multiple_replace_template
)
from src.srt_utils import save_srt, sub_midpoints_ms, clean_srt_text
from src.video import open_video, get_frame_at_ms, encode_jpg_bytes
from src.models import TextModel, VisionModel
from src.local_vision_model import LocalVisionModel
from src.audio_model import AudioModel
from src.subtitle_item import create_subtitle_items_from_srt, SubtitleItem
from src.pipeline_runs import (
    run_audio,
    run_brief_text,
    run_vision_single,
    run_vision_multi,
    run_context_expansion,
    run_final_translate,
)
from src.jsonl_compat import (
    load_audio_results_compat,
    load_brief_results_compat,
    load_vision_results_compat,
    load_final_translations_compat,
)
from src.pipeline import parse_pack_from_reasons

# Paths relative to app directory so they work regardless of CWD
_APP_DIR = Path(__file__).resolve().parent
CONFIG_PATH = _APP_DIR / "config.json"
PO_DIR = _APP_DIR / "po"
PROMPTS_CSV_PATH = _APP_DIR / "model_prompts.csv"
RUN_E_PROMPTS_CSV_PATH = _APP_DIR / "model_prompts_run_f.csv"  # Run F ç¿»è­¯ç”¨ï¼šmain_group_translate, main_assemble, local_polish, localization
LANG_CONFIG_PATH = _APP_DIR / "language_config.json"

# Async batching configuration
BATCH_SIZE_STAGE2 = 32
BATCH_SIZE_STAGE3 = 32
MAX_CONCURRENT_BATCHES = 2  # Control GPU memory pressure


def _cache_pack_ok(loaded_items: dict, version: str, min_ratio: float = 0.90) -> bool:
    """
    Check if cached brief_{version} has PACK in reasons for enough items.
    Old cache may have reasons without PACK, causing pack=None in Run F.
    Returns True only if at least min_ratio (default 90%) of items with brief_vX have parseable PACK.
    """
    if not loaded_items or version not in ("v1", "v2", "v3"):
        return False
    attr = "brief_v1" if version == "v1" else "brief_v2" if version == "v2" else "brief_v3"
    total = 0
    ok = 0
    for item in loaded_items.values():
        brief = getattr(item, attr, None)
        if brief is None:
            continue
        total += 1
        if parse_pack_from_reasons(brief.reasons or "") is not None:
            ok += 1
    if total == 0:
        return False
    return (ok / total) >= min_ratio


def _copy_brief_snapshot(work_dir: Path, snapshot_version: str) -> None:
    """å–®ä¸€ briefï¼šåœ¨éšæ®µæ›´æ–°å‰å°‡ç•¶å‰ brief_work.jsonl è¤‡è£½ç‚º snapshotï¼ˆbrief_v1/v2/v3/v4.jsonlï¼‰ã€‚"""
    # æ–°ç‰ˆä¸»æª”åç‚º brief_work.jsonlï¼›ç‚ºç›¸å®¹èˆŠç‰ˆï¼Œè‹¥ä¸å­˜åœ¨å‰‡å›é€€è‡³ brief.jsonlã€‚
    src = work_dir / "brief_work.jsonl"
    if not src.exists():
        src = work_dir / "brief.jsonl"
    dst = work_dir / f"brief_{snapshot_version}.jsonl"
    if src.exists():
        shutil.copy2(src, dst)


def _load_available_locales_from_csv(csv_path: Path) -> list[str]:
    """
    ï¼ˆä¿ç•™çµ¦æœªä¾†æ“´å……ç”¨ï¼Œç›®å‰ä¸å†å¾ CSV è®€èªè¨€ç¢¼ï¼‰
    ç‚ºäº†ç›¸å®¹èˆŠç¨‹å¼ç¢¼ï¼Œç¾åœ¨æ°¸é å›å‚³ç©ºåˆ—è¡¨ï¼Œå¯¦éš›èªè¨€è¨­å®šæ”¹ç”± language_config.json ç®¡ç†ã€‚
    """
    return []


@dataclass
class ModelPromptConfig:
    """å¾ CSV è¼‰å…¥çš„æ¨¡å‹ prompt è¨­å®š"""
    model_name: str
    role: str  # "main", "localization", "vision"
    chat_format: str
    system_prompt_template: str
    user_prompt_template: str
    batch_user_prompt_template: str = ""  # optional; for role=main, batch brief (multi-item in one request)


def _load_prompt_from_csv_by_model_name(
    csv_path: Path,
    model_filename: str,
    role: str,  # "main", "main_assemble", "main_group_translate", "localization", "local_polish"
) -> ModelPromptConfig | None:
    """
    å¾ CSV ä¾ model_name åŒ¹é…è¼‰å…¥ prompt è¨­å®šã€‚
    
    åŒ¹é…è¦å‰‡ï¼šåªè¦ model_filenameï¼ˆä¸åˆ†å¤§å°å¯«ï¼‰åŒ…å« CSV ä¸­çš„ model_nameï¼ˆä¸åˆ†å¤§å°å¯«ï¼‰ï¼Œ
    å°±è¦–ç‚ºåŒ¹é…æˆåŠŸã€‚è¿”å›ç¬¬ä¸€å€‹åŒ¹é…çš„ rowã€‚
    
    Args:
        csv_path: CSV æª”æ¡ˆè·¯å¾‘
        model_filename: å¯¦éš›æ¨¡å‹æª”åï¼ˆä¾‹å¦‚ "Breeze-7B-Instruct-v1_0.Q5_K_M.gguf"ï¼‰
        role: è¦åŒ¹é…çš„è§’è‰²ï¼ˆ"main"ã€"main_assemble"ã€"main_group_translate" ç¶ä¸»æ¨¡å‹ï¼›"localization"ã€"local_polish" ç¶åœ¨åœ°åŒ–æ¨¡å‹ï¼‰
    
    Returns:
        ModelPromptConfig æˆ– Noneï¼ˆæ‰¾ä¸åˆ°åŒ¹é…æ™‚ï¼‰
    """
    if not csv_path.exists():
        return None
    
    model_lower = model_filename.lower()
    
    try:
        with csv_path.open("r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            for row in reader:
                csv_role = (row.get("role") or "").strip()
                if csv_role != role:
                    continue
                
                csv_model_name = (row.get("model_name") or "").strip()
                if not csv_model_name:
                    continue
                
                # åŒ¹é…ï¼šmodel_filenameï¼ˆä¸åˆ†å¤§å°å¯«ï¼‰åŒ…å« CSV çš„ model_nameï¼ˆä¸åˆ†å¤§å°å¯«ï¼‰
                if csv_model_name.lower() in model_lower:
                    return ModelPromptConfig(
                        model_name=csv_model_name,
                        role=csv_role,
                        chat_format=(row.get("chat_format") or "chatml").strip(),
                        system_prompt_template=(row.get("system_prompt_template") or "").strip(),
                        user_prompt_template=(row.get("user_prompt_template") or "").strip(),
                        batch_user_prompt_template=(row.get("batch_user_prompt_template") or "").strip(),
                    )
    except Exception:
        return None
    
    return None


def _load_run_e_prompt_from_csv(model_filename: str, role: str) -> ModelPromptConfig | None:
    """
    Run F ç¿»è­¯ç”¨ï¼šå„ªå…ˆå¾ model_prompts_run_e.csv è¼‰å…¥ promptï¼›è‹¥ç„¡æª”æ¡ˆæˆ–ç„¡åŒ¹é…å‰‡å›é€€åˆ° model_prompts.csvã€‚
    role æ‡‰ç‚º main_group_translate, main_assemble, local_polish, localization ä¹‹ä¸€ã€‚
    """
    if RUN_E_PROMPTS_CSV_PATH.exists():
        cfg = _load_prompt_from_csv_by_model_name(RUN_E_PROMPTS_CSV_PATH, model_filename, role)
        if cfg is not None:
            return cfg
    return _load_prompt_from_csv_by_model_name(PROMPTS_CSV_PATH, model_filename, role)


def _load_language_prefs_from_csv(csv_path: Path) -> tuple[str, str, list[str]]:
    """
    å¾ model_prompts.csv æ¨æ–·èªè¨€åå¥½ï¼ˆèˆŠé‚è¼¯ï¼Œç”¨æ–¼ fallbackï¼Œç¾éšæ®µé€šå¸¸ä¸æœƒç”Ÿæ•ˆï¼‰ï¼š
      - éå»æœƒè®€å– role == \"localization\" çš„ `target_language` æ¬„ä½æ±ºå®šèªè¨€ï¼›
      - ç›®å‰å·²ç§»é™¤è©²æ¬„ä½ï¼Œæ­¤å‡½å¼å¤šåŠå›å‚³é è¨­å€¼ï¼Œå¯¦éš›èªè¨€è¨­å®šç”± language_config.json ç®¡ç†ã€‚
    """
    ui_locale = "zh-TW"
    target_locale = "zh-TW"
    available_locales: list[str] = []
    try:
        if not csv_path.exists():
            return ui_locale, target_locale, available_locales
        with csv_path.open("r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            for row in reader:
                role = (row.get("role") or "").strip()
                if role != "localization":
                    continue
                target = (row.get("target_language") or "").strip()
                if target:
                    if not available_locales:
                        ui_locale = target
                        target_locale = target
                    if target not in available_locales:
                        available_locales.append(target)
    except Exception:
        return ui_locale, target_locale, available_locales
    return ui_locale, target_locale, available_locales


def _load_language_prefs(lang_cfg_path: Path, csv_path: Path) -> tuple[str, str, list[str]]:
    """
    å¾ç¨ç«‹çš„èªè¨€è¨­å®šæª”è¼‰å…¥ï¼š
      - ui_locale: UI ä»‹é¢èªè¨€ï¼ˆå°æ‡‰ .poï¼‰
      - target_locale_default: é è¨­ç¿»è­¯ç›®æ¨™èªè¨€
      - available_target_locales: ç›®æ¨™èªè¨€ä¸‹æ‹‰é¸å–®å¯é¸æ¸…å–®

    ä»‹é¢èªè¨€èˆ‡ç›®æ¨™èªè¨€ä»¥ language_config.json ç‚ºæº–ï¼›åƒ…åœ¨ JSON ä¸å­˜åœ¨æˆ–è®€å–å¤±æ•—æ™‚ï¼Œ
    æ‰ç”¨ CSV æ¨æ–·ã€‚é€™æ¨£ä¿®æ”¹ language_config.json çš„ ui_locale å³å¯åˆ‡æ›ä»‹é¢èªè¨€ã€‚
    """
    # å…ˆçµ¦ä¸€çµ„åˆç†çš„é è¨­ï¼ˆç¹é«”ä¸­æ–‡ï¼‰
    ui_locale = "zh-TW"
    target_locale = "zh-TW"
    available_locales: list[str] = ["zh-TW", "zh-CN", "ja-JP", "es-ES"]
    json_loaded = False

    # å„ªå…ˆå¾ language_config.json è¼‰å…¥ï¼ˆä»‹é¢èªè¨€èˆ‡ç›®æ¨™èªè¨€ä»¥æ­¤ç‚ºæº–ï¼‰
    try:
        if lang_cfg_path.exists():
            data = json.loads(lang_cfg_path.read_text(encoding="utf-8"))
            if data is not None:
                ui_locale = (data.get("ui_locale") or ui_locale).strip()
                target_locale = (data.get("target_locale_default") or target_locale).strip()
                cfg_locales = data.get("available_target_locales")
                if isinstance(cfg_locales, list) and cfg_locales:
                    available_locales = [str(x).strip() for x in cfg_locales if str(x).strip()]
                json_loaded = True
    except Exception:
        pass

    # åƒ…ç•¶æœªæˆåŠŸè¼‰å…¥ JSON æ™‚ï¼Œæ‰ç”¨ CSV æ¨æ–·ï¼ˆé¿å… CSV è¦†å¯« JSON çš„ä»‹é¢èªè¨€ï¼‰
    csv_ui, csv_target, csv_locales = _load_language_prefs_from_csv(csv_path)
    if not json_loaded:
        if csv_locales:
            available_locales = csv_locales
        if csv_ui:
            ui_locale = csv_ui
        if csv_target:
            target_locale = csv_target

    return ui_locale, target_locale, available_locales


_UI_LOCALE, _TARGET_LANG_LOCALE, _AVAILABLE_TARGET_LOCALES = _load_language_prefs(
    LANG_CONFIG_PATH, PROMPTS_CSV_PATH
)


def _parse_po_string(s: str) -> str:
    """Unescape .po string (strip outer quotes, handle \\ \\n \\")."""
    s = s.strip()
    if len(s) >= 2 and s.startswith('"') and s.endswith('"'):
        s = s[1:-1]
    return s.replace("\\n", "\n").replace("\\\"", '"').replace("\\\\", "\\")


def _load_po_catalog(po_dir: Path, locale: str) -> dict[str, str]:
    """
    Load translations from po/{locale}.po (flat .po files).
    gettext expects po/{locale}/LC_MESSAGES/messages.mo; we use flat .po instead
    so interface language follows language_config.json ui_locale without .mo build.
    Returns dict msgid -> msgstr. Empty msgid (header) is skipped.
    """
    catalog: dict[str, str] = {}
    po_path = po_dir / f"{locale}.po"
    if not po_path.exists():
        return catalog
    try:
        text = po_path.read_text(encoding="utf-8")
    except Exception:
        return catalog
    current_msgid: list[str] = []
    current_msgstr: list[str] = []
    in_msgid = False
    in_msgstr = False

    for line in text.splitlines():
        stripped = line.strip()
        if stripped.startswith("msgid "):
            if in_msgstr and current_msgid:
                key = "".join(current_msgid)
                val = "".join(current_msgstr)
                if key != "":
                    catalog[key] = val
            current_msgid = [_parse_po_string(stripped[6:].strip())]
            current_msgstr = []
            in_msgid = True
            in_msgstr = False
        elif stripped.startswith("msgstr "):
            current_msgstr = [_parse_po_string(stripped[7:].strip())]
            in_msgid = False
            in_msgstr = True
        elif stripped.startswith('"') and (in_msgid or in_msgstr):
            if in_msgid:
                current_msgid.append(_parse_po_string(stripped))
            else:
                current_msgstr.append(_parse_po_string(stripped))
    if current_msgid and in_msgstr:
        key = "".join(current_msgid)
        val = "".join(current_msgstr)
        if key != "":
            catalog[key] = val
    return catalog


_PO_CATALOG = _load_po_catalog(PO_DIR, _UI_LOCALE)


def _(s: str) -> str:
    """Translate string using language_config.json ui_locale and po/{locale}.po."""
    if not s:
        return s
    return _PO_CATALOG.get(s, s)


def _as_path(x: Any) -> Optional[str]:
    """Best-effort to extract a local file path from Gradio inputs."""
    if x is None:
        return None
    if isinstance(x, str):
        return x
    # common Gradio objects
    if hasattr(x, "path") and isinstance(getattr(x, "path"), str):
        return getattr(x, "path")
    if hasattr(x, "name") and isinstance(getattr(x, "name"), str):
        return getattr(x, "name")
    if isinstance(x, dict):
        p = x.get("path") or x.get("name")
        if isinstance(p, str):
            return p
    try:
        return str(x)
    except Exception:
        return None


# -----------------------------
# Helpers
# -----------------------------
def _ensure_config() -> AppConfig:
    if not CONFIG_PATH.exists():
        # create minimal config with placeholders
        CONFIG_PATH.write_text(json.dumps({"models_dir": "./models"}, indent=2), encoding="utf-8")
    return load_config(CONFIG_PATH)


def _model_path(models_dir: str, filename: str) -> str:
    """Legacy helper: join models_dir + filename."""
    p = Path(models_dir) / filename
    return str(p.resolve())


def _pick_gguf_from_dir(dir_path: Path) -> Optional[Path]:
    """
    Smart GGUF picker for a directory.

    Rules:
      - If there is exactly one .gguf file, use it.
      - If there are multiple .gguf files, assume they are shards of the SAME model.
        In that case:
          * Prefer the shard file matching '*-00001-of-*.gguf'
          * Otherwise, fall back to the largest .gguf file.
    
    Note: å¯¦éš›å¯¦ä½œåœ¨ src.model_path_utils.pick_gguf_from_dirï¼Œé€™è£¡æ˜¯ç›¸å®¹æ€§åŒ…è£ã€‚
    """
    from src.model_path_utils import pick_gguf_from_dir
    return pick_gguf_from_dir(dir_path)


def _resolve_reason_model_path(cfg: AppConfig) -> Path:
    """
    Resolve the GGUF path for the 'reason' model (Stage 2).

    Priority:
      1) If cfg.reason_dir is set and has GGUFs, pick from there.
      2) Otherwise, fall back to models_dir + qwen_model (legacy).
    
    Note: å¯¦éš›å¯¦ä½œåœ¨ src.model_path_utils.resolve_reason_model_path
    """
    from src.model_path_utils import resolve_reason_model_path
    return resolve_reason_model_path(cfg)


def _resolve_translate_model_path(cfg: AppConfig) -> Path:
    """
    Resolve the GGUF path for the 'translate' model (Stage 3).

    Priority:
      1) If cfg.translate_dir is set and has GGUFs, pick from there.
      2) Otherwise, fall back to models_dir + breeze_model (legacy).
    
    Note: å¯¦éš›å¯¦ä½œåœ¨ src.model_path_utils.resolve_translate_model_path
    """
    from src.model_path_utils import resolve_translate_model_path
    return resolve_translate_model_path(cfg)


def _check_vision_assets(cfg: AppConfig) -> tuple[bool, list[str]]:
    """
    æª¢æŸ¥ vision æ¨¡å‹æª”æ¡ˆæ˜¯å¦å­˜åœ¨ã€‚
    
    Args:
        cfg: AppConfig å¯¦ä¾‹
    
    Returns:
        (ok: bool, missing: list[str])
        - ok: True è¡¨ç¤ºæ‰€æœ‰æª”æ¡ˆéƒ½å­˜åœ¨
        - missing: ç¼ºå°‘çš„æª”æ¡ˆè·¯å¾‘åˆ—è¡¨ï¼ˆç”¨æ–¼éŒ¯èª¤æç¤ºï¼‰
    """
    text_p, mmproj_p, _ = _resolve_vision_paths(cfg)
    missing = []
    
    if text_p is None or not text_p.exists():
        expected_path = Path(cfg.models_dir) / cfg.vision.text_model
        if getattr(cfg, "vision_text_dir", None):
            expected_path = Path(cfg.vision_text_dir) / "*.gguf"
        missing.append(f"Vision text model: {expected_path}")
    
    if mmproj_p is None or not mmproj_p.exists():
        expected_path = Path(cfg.models_dir) / cfg.vision.mmproj_model
        if getattr(cfg, "vision_mmproj_dir", None):
            expected_path = Path(cfg.vision_mmproj_dir) / "*.gguf"
        missing.append(f"Vision mmproj model: {expected_path}")
    
    return len(missing) == 0, missing


def _resolve_vision_paths(cfg: AppConfig) -> tuple[Optional[Path], Optional[Path], Optional[str]]:
    """
    Resolve GGUF paths for vision text + mmproj models and detect model type.

    Directory-based layout:
      - cfg.vision_text_dir: directory containing vision text model GGUFs
      - cfg.vision_mmproj_dir: directory containing vision mmproj GGUFs

    Fallback:
      - Use models_dir + cfg.vision.text_model / cfg.vision.mmproj_model
      - Auto-detect model type based on filename patterns
      - Supports all llama-cpp-python vision models (Moondream2, LLaVA, BakLLaVA, etc.)

    Returns:
        (text_model_path, mmproj_path, model_type)
        model_type: detected model type string or None (will be auto-detected by LocalVisionModel)
    
    Note: å¯¦éš›å¯¦ä½œåœ¨ src.model_path_utils.resolve_vision_paths
    """
    from src.model_path_utils import resolve_vision_paths
    return resolve_vision_paths(cfg)


def _check_models(cfg: AppConfig):
    missing = []
    # Reason model
    reason_p = _resolve_reason_model_path(cfg)
    if not reason_p.exists():
        if cfg.reason_dir:
            missing.append(f"Reason model missing in directory: {cfg.reason_dir} (no *.gguf found)")
        else:
            missing.append(f"Reason model missing: {reason_p}")

    # Translate model
    translate_p = _resolve_translate_model_path(cfg)
    if not translate_p.exists():
        if cfg.translate_dir:
            missing.append(f"Translate model missing in directory: {cfg.translate_dir} (no *.gguf found)")
        else:
            missing.append(f"Translate model missing: {translate_p}")

    if cfg.vision.enabled:
        v_text, v_mmproj, v_type = _resolve_vision_paths(cfg)

        if not v_text.exists():
            missing.append(f"Vision text model missing: {v_text}")
        if not v_mmproj.exists():
            missing.append(f"Vision mmproj missing: {v_mmproj}")
    return missing


def _gradio_video_path(video_value) -> str | None:
    """
    Gradio Video input returns a filepath string by default.
    But we defensively handle dict-like or object-like values too.
    """
    if video_value is None:
        return None
    if isinstance(video_value, str):
        return video_value
    # Some gradio versions/components may return dict-like
    if isinstance(video_value, dict):
        return video_value.get("name") or video_value.get("path") or video_value.get("data")
    # Fallback: try .name
    return getattr(video_value, "name", None)


def _gradio_file_path(file_obj) -> str | None:
    """
    Gradio File(type="file") usually returns a temporary file object with `.name` path.
    """
    if file_obj is None:
        return None
    if isinstance(file_obj, str):
        return file_obj
    if isinstance(file_obj, dict):
        return file_obj.get("name") or file_obj.get("path")
    return getattr(file_obj, "name", None)


def _read_uploaded_file_bytes(file_obj) -> bytes:
    """
    Robustly read bytes from Gradio uploaded file object.
    """
    if file_obj is None:
        return b""
    # If it's already bytes:
    if isinstance(file_obj, (bytes, bytearray)):
        return bytes(file_obj)
    # If dict with path:
    if isinstance(file_obj, dict):
        p = file_obj.get("name") or file_obj.get("path")
        if p:
            return Path(p).read_bytes()
        return b""
    # file-like
    try:
        file_obj.seek(0)
    except Exception:
        pass
    try:
        return file_obj.read()
    except Exception:
        # last resort by path
        p = getattr(file_obj, "name", None)
        if p:
            return Path(p).read_bytes()
        return b""


def _save_preview_bytes_to_file(img_bytes: bytes, work_dir: Path) -> str:
    """Save preview image bytes to work_dir and return path string for Gradio Image component."""
    if not img_bytes:
        return None
    try:
        preview_path = work_dir / "preview_frame.jpg"
        preview_path.write_bytes(img_bytes)
        return str(preview_path)
    except Exception:
        return None


# -----------------------------
# Async Dynamic Batched Inference
# -----------------------------

@dataclass
class LineTask:
    """å–®è¡Œå­—å¹•çš„å®Œæ•´ä»»å‹™ä¸Šä¸‹æ–‡ï¼Œç”¨æ–¼æ‰¹æ¬¡è™•ç†"""
    index: int  # åœ¨ subs ä¸­çš„ç´¢å¼•
    sub: pysrt.SubRipItem  # SRT é …ç›®
    line_en_raw: str  # æ¸…ç†å¾Œçš„è‹±æ–‡åŸæ–‡
    prev_ctx: list[str] = field(default_factory=list)  # Round 1 ä¸Šä¸‹æ–‡ï¼ˆå‰ä¸€å¥ï¼‰
    next_ctx: list[str] = field(default_factory=list)  # Round 1 ä¸Šä¸‹æ–‡ï¼ˆå¾Œä¸€å¥ï¼‰
    more_prev: list[str] = field(default_factory=list)  # Round 2 æ“´å±•ä¸Šä¸‹æ–‡ï¼ˆå‰4å¥ï¼‰
    more_next: list[str] = field(default_factory=list)  # Round 2 æ“´å±•ä¸Šä¸‹æ–‡ï¼ˆå¾Œ4å¥ï¼‰
    
    # Round 1 çµæœ
    s2_round1: Optional[Stage2Result] = None
    
    # Round 2 çµæœï¼ˆå¦‚æœåŸ·è¡Œï¼‰
    s2_round2: Optional[Stage2Result] = None
    
    # Vision çµæœï¼ˆå¦‚æœåŸ·è¡Œï¼‰
    visual_hint: Optional[str] = None
    s2_vision: Optional[Stage2Result] = None
    
    # æœ€çµ‚ä½¿ç”¨çš„çµæœ
    final_s2: Optional[Stage2Result] = None
    
    # Stage3 çµæœ
    zh_output: Optional[str] = None


async def run_stage2_batch(
    reason_model: TextModel,
    tasks: list[LineTask],
    semaphore: asyncio.Semaphore,
    use_more_context: bool = False,
    visual_hint_map: dict[int, str] | None = None,
    prompt_config: ModelPromptConfig | None = None,
) -> list[Stage2Result]:
    """
    æ‰¹æ¬¡åŸ·è¡Œ Stage2 æ¨ç†ã€‚
    
    Args:
        reason_model: Stage2 æ¨¡å‹
        tasks: è¦è™•ç†çš„ LineTask åˆ—è¡¨
        semaphore: æ§åˆ¶ä½µç™¼æ•¸
        use_more_context: æ˜¯å¦ä½¿ç”¨æ“´å±•ä¸Šä¸‹æ–‡ï¼ˆRound 2ï¼‰
        visual_hint_map: {index: visual_hint} å°æ‡‰è¡¨ï¼ˆRound 4ï¼‰
    
    Returns:
        å°æ‡‰é †åºçš„ Stage2Result åˆ—è¡¨
    """
    async def process_one(task: LineTask) -> Stage2Result:
        async with semaphore:
            # é¸æ“‡ä¸Šä¸‹æ–‡
            if use_more_context:
                prev_lines = task.more_prev
                next_lines = task.more_next
            else:
                prev_lines = task.prev_ctx
                next_lines = task.next_ctx
            
            # é¸æ“‡ visual_hint
            visual_hint = None
            if visual_hint_map and task.index in visual_hint_map:
                visual_hint = visual_hint_map[task.index]
            
            # åœ¨åŸ·è¡Œç·’æ± ä¸­åŸ·è¡ŒåŒæ­¥çš„ stage2_reason_and_score
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                stage2_reason_and_score,
                reason_model,
                task.line_en_raw,
                prev_lines,
                next_lines,
                visual_hint,
                prompt_config,  # å‚³å…¥ CSV è¼‰å…¥çš„ prompt è¨­å®š
            )
            return result
    
    # ä¸¦ç™¼åŸ·è¡Œæ‰€æœ‰ä»»å‹™
    results = await asyncio.gather(*[process_one(task) for task in tasks])
    return list(results)


async def run_stage3_batch(
    translate_model: TextModel,
    tasks: list[LineTask],
    prev_zh_map: dict[int, list[str]],  # {index: [prev_zh_lines]}
    glossary: list[GlossaryEntry],
    target_language: str,
    semaphore: asyncio.Semaphore,
    prompt_config: ModelPromptConfig | None = None,
) -> list[str]:
    """
    æ‰¹æ¬¡åŸ·è¡Œ Stage3 ç¿»è­¯ã€‚
    
    Args:
        translate_model: Stage3 æ¨¡å‹
        tasks: è¦è™•ç†çš„ LineTask åˆ—è¡¨ï¼ˆå¿…é ˆå·²æœ‰ final_s2ï¼‰
        prev_zh_map: {index: [prev_zh_lines]} å‰æ–‡ä¸­æ–‡å°æ‡‰
        glossary: è©å½™è¡¨
        target_language: ç›®æ¨™èªè¨€æ¨™ç±¤
        semaphore: æ§åˆ¶ä½µç™¼æ•¸
    
    Returns:
        å°æ‡‰é †åºçš„ä¸­æ–‡ç¿»è­¯åˆ—è¡¨
    """
    async def process_one(task: LineTask) -> str:
        async with semaphore:
            if task.final_s2 is None:
                raise ValueError(f"Task {task.index} missing final_s2")
            
            prev_zh = prev_zh_map.get(task.index, [])
            
            # åœ¨åŸ·è¡Œç·’æ± ä¸­åŸ·è¡ŒåŒæ­¥çš„ stage3_translate
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                stage3_translate,
                translate_model,
                task.line_en_raw,
                task.final_s2.meaning_en,
                prev_zh,
                glossary,
                target_language,
                prompt_config,  # å‚³å…¥ CSV è¼‰å…¥çš„ prompt è¨­å®š
            )
            return result
    
    # ä¸¦ç™¼åŸ·è¡Œæ‰€æœ‰ä»»å‹™
    results = await asyncio.gather(*[process_one(task) for task in tasks])
    return list(results)


# -----------------------------
# Model Manager (ç¢ºä¿ä»»ä¸€æ™‚é–“åªè¼‰å…¥ä¸€å€‹æ¨¡å‹)
# -----------------------------
class ModelManager:
    """ç®¡ç†æ¨¡å‹è¼‰å…¥/å¸è¼‰ï¼Œç¢ºä¿ä»»ä¸€æ™‚é–“åªå­˜åœ¨ä¸€å€‹æ¨¡å‹å¯¦ä¾‹"""
    
    def __init__(self, log_lines_ref: list[str]):
        """åˆå§‹åŒ–ï¼Œéœ€è¦ log_lines çš„å¼•ç”¨"""
        self.current_model: Any | None = None
        self.current_model_type: str | None = None  # "audio", "reason", "vision", "translate"
        self.log_lines = log_lines_ref
    
    def _assert_no_model_loaded(self, operation: str):
        """å…§éƒ¨æª¢æŸ¥ï¼šç¢ºä¿æ²’æœ‰æ¨¡å‹å·²è¼‰å…¥ï¼ˆç”¨æ–¼èª¿è©¦ï¼‰"""
        if self.current_model is not None:
            self.log_lines.append(
                f"[ModelManager] âš ï¸ è­¦å‘Šï¼šåœ¨ {operation} æ™‚ç™¼ç¾å·²è¼‰å…¥çš„æ¨¡å‹ "
                f"({self.current_model_type})ï¼Œå°‡å…ˆå¸è¼‰"
            )
            self.unload_all()
    
    def load_reason_model(self, cfg: AppConfig, prompt_config: Any | None) -> TextModel:
        """è¼‰å…¥ä¸»æ¨ç†æ¨¡å‹ï¼ˆå¸è¼‰å…¶ä»–æ¨¡å‹ï¼‰ï¼Œå„ªå…ˆä½¿ç”¨ GPU åŠ é€Ÿ"""
        # åš´æ ¼æª¢æŸ¥ï¼šç¢ºä¿æ²’æœ‰å…¶ä»–æ¨¡å‹å·²è¼‰å…¥
        self._assert_no_model_loaded("load_reason_model")
        self.unload_all()  # é›™é‡ä¿éšª
        
        self.log_lines.append("[ModelManager] ğŸ”„ Unloading all models, preparing to load main reasoning model...")
        reason_path = _resolve_reason_model_path(cfg)
        reason_chat_format = prompt_config.chat_format if prompt_config else "chatml"
        
        try:
            model = TextModel(
                model_path=str(reason_path),
                chat_format=reason_chat_format,
                n_ctx=cfg.llama_cpp.n_ctx_reason,
                n_gpu_layers=cfg.llama_cpp.n_gpu_layers_reason,  # æœƒè¢« TextModel å¼·åˆ¶è¨­ç‚º -1ï¼ˆGPUï¼‰
                n_threads=cfg.llama_cpp.n_threads,
            )
            self.current_model = model
            self.current_model_type = "reason"
            self.log_lines.append("[ModelManager] âœ“ Main reasoning model loaded successfully (GPU acceleration: enabled)")
            return model
        except Exception as e:
            # å¦‚æœè¼‰å…¥å¤±æ•—ï¼Œç¢ºä¿ç‹€æ…‹æ¸…ç†
            self.current_model = None
            self.current_model_type = None
            raise
    
    def load_translate_model(self, cfg: AppConfig, prompt_config: Any | None) -> TextModel:
        """è¼‰å…¥ç¿»è­¯æ¨¡å‹ï¼ˆå¸è¼‰å…¶ä»–æ¨¡å‹ï¼‰ï¼Œå„ªå…ˆä½¿ç”¨ GPU åŠ é€Ÿ"""
        # åš´æ ¼æª¢æŸ¥ï¼šç¢ºä¿æ²’æœ‰å…¶ä»–æ¨¡å‹å·²è¼‰å…¥
        self._assert_no_model_loaded("load_translate_model")
        self.unload_all()  # é›™é‡ä¿éšª
        
        self.log_lines.append("[ModelManager] ğŸ”„ Unloading all models, preparing to load translation model...")
        translate_path = _resolve_translate_model_path(cfg)
        translate_chat_format = prompt_config.chat_format if prompt_config else "chatml"
        
        try:
            model = TextModel(
                model_path=str(translate_path),
                chat_format=translate_chat_format,
                n_ctx=cfg.llama_cpp.n_ctx_translate,
                n_gpu_layers=cfg.llama_cpp.n_gpu_layers_translate,  # æœƒè¢« TextModel å¼·åˆ¶è¨­ç‚º -1ï¼ˆGPUï¼‰
                n_threads=cfg.llama_cpp.n_threads,
            )
            self.current_model = model
            self.current_model_type = "translate"
            self.log_lines.append("[ModelManager] âœ“ Translation model loaded successfully (GPU acceleration: enabled)")
            return model
        except Exception as e:
            # å¦‚æœè¼‰å…¥å¤±æ•—ï¼Œç¢ºä¿ç‹€æ…‹æ¸…ç†
            self.current_model = None
            self.current_model_type = None
            raise
    
    def load_vision_model(self, cfg: AppConfig) -> LocalVisionModel:
        """è¼‰å…¥è¦–è¦ºæ¨¡å‹ï¼ˆå¸è¼‰å…¶ä»–æ¨¡å‹ï¼‰ï¼Œè‡ªå‹•æª¢æ¸¬æ¨¡å‹é¡å‹ï¼ˆæ”¯æ´æ‰€æœ‰ llama-cpp-python è¦–è¦ºæ¨¡å‹ï¼‰ï¼Œå„ªå…ˆä½¿ç”¨ GPU åŠ é€Ÿ"""
        # åš´æ ¼æª¢æŸ¥ï¼šç¢ºä¿æ²’æœ‰å…¶ä»–æ¨¡å‹å·²è¼‰å…¥
        self._assert_no_model_loaded("load_vision_model")
        self.unload_all()  # é›™é‡ä¿éšª
        
        self.log_lines.append("[ModelManager] ğŸ”„ Unloading all models, preparing to load vision model...")
        vision_text_p, vision_mmproj_p, model_type = _resolve_vision_paths(cfg)
        
        if not vision_text_p or not vision_text_p.exists():
            raise FileNotFoundError(f"Vision text model not found. Searched in: {cfg.models_dir}")
        if not vision_mmproj_p or not vision_mmproj_p.exists():
            raise FileNotFoundError(f"Vision mmproj model not found. Searched in: {cfg.models_dir}")
        
        try:
            # LocalVisionModel æœƒè‡ªå‹•æª¢æ¸¬æ¨¡å‹é¡å‹ä¸¦é¸æ“‡åˆé©çš„ ChatHandler
            # é€™è£¡å¯ä»¥é¸æ“‡æ€§åœ°å‚³å…¥ model_type ä»¥åŠ é€Ÿæª¢æ¸¬ï¼Œæˆ–è®“å®ƒè‡ªå‹•æª¢æ¸¬
            # LocalVisionModel å…§éƒ¨æœƒè¨­ç½® n_gpu_layers=-1 ä»¥å„ªå…ˆä½¿ç”¨ GPU
            model = LocalVisionModel(
                model_path=str(vision_text_p.resolve()),
                clip_model_path=str(vision_mmproj_p.resolve()),
                model_type=model_type,  # å¦‚æœç‚º Noneï¼Œæœƒè‡ªå‹•æª¢æ¸¬
                n_ctx=None,  # è®“ LocalVisionModel æ ¹æ“šæ¨¡å‹é¡å‹è‡ªå‹•è¨­å®š
                n_threads=cfg.llama_cpp.n_threads,
            )
            self.current_model = model
            self.current_model_type = "vision"
            detected_type = getattr(model, 'model_type', 'auto')
            self.log_lines.append(f"[ModelManager] âœ“ Vision model loaded successfully (type: {detected_type}, GPU acceleration: enabled)")
            return model
        except Exception as e:
            # å¦‚æœè¼‰å…¥å¤±æ•—ï¼Œç¢ºä¿ç‹€æ…‹æ¸…ç†
            self.current_model = None
            self.current_model_type = None
            raise
    
    def load_audio_model(self, cfg: AppConfig) -> AudioModel:
        """è¼‰å…¥éŸ³è¨Šæ¨¡å‹ï¼ˆå¸è¼‰å…¶ä»–æ¨¡å‹ï¼‰"""
        # åš´æ ¼æª¢æŸ¥ï¼šç¢ºä¿æ²’æœ‰å…¶ä»–æ¨¡å‹å·²è¼‰å…¥
        self._assert_no_model_loaded("load_audio_model")
        self.unload_all()  # é›™é‡ä¿éšª
        
        self.log_lines.append("[ModelManager] ğŸ”„ Unloading all models, preparing to load audio model...")
        audio_model_dir = Path(cfg.audio.model_dir)
        if not audio_model_dir.exists():
            raise FileNotFoundError(f"Audio model directory not found: {audio_model_dir}")
        
        try:
            model = AudioModel(audio_model_dir)
            self.current_model = model
            self.current_model_type = "audio"
            self.log_lines.append("[ModelManager] âœ“ Audio model loaded successfully")
            return model
        except Exception as e:
            # å¦‚æœè¼‰å…¥å¤±æ•—ï¼Œç¢ºä¿ç‹€æ…‹æ¸…ç†
            self.current_model = None
            self.current_model_type = None
            raise
    
    def unload_all(self):
        """å¸è¼‰ç•¶å‰æ¨¡å‹ï¼ˆå¼·åˆ¶é‡‹æ”¾è³‡æºï¼‰"""
        if self.current_model is not None:
            model_type_str = self.current_model_type or "unknown"
            self.log_lines.append(f"[ModelManager] ğŸ”„ Unloading {model_type_str} model...")
            
            try:
                # å°æ–¼ llama-cpp-python æ¨¡å‹ï¼Œå˜—è©¦æ‰‹å‹•é‡‹æ”¾
                if hasattr(self.current_model, 'llm'):
                    # TextModel æˆ– LocalVisionModel
                    llm = self.current_model.llm
                    if hasattr(llm, 'free'):
                        try:
                            llm.free()  # å¦‚æœ llama-cpp-python æä¾› free æ–¹æ³•
                        except Exception:
                            pass
                
                # éŸ³è¨Šæ¨¡å‹ï¼ˆHF pipelineï¼‰éš¨ del é‡‹æ”¾ï¼Œç„¡éœ€é¡å¤–æ¸…ç†

                # å¼·åˆ¶åˆªé™¤å¼•ç”¨
                del self.current_model
                self.current_model = None
                self.current_model_type = None
                
                # å¼·åˆ¶åƒåœ¾å›æ”¶ï¼ˆå¯é¸ï¼Œä½†å¯èƒ½æœ‰åŠ©æ–¼ç«‹å³é‡‹æ”¾ VRAMï¼‰
                import gc
                gc.collect()
                
                self.log_lines.append(f"[ModelManager] âœ“ {model_type_str} model unloaded (resources released)")
            except Exception as e:
                self.log_lines.append(f"[ModelManager] âš ï¸ Error occurred while unloading model (but references cleared): {e}")
                # å³ä½¿å‡ºéŒ¯ï¼Œä¹Ÿè¦æ¸…é™¤å¼•ç”¨
                self.current_model = None
                self.current_model_type = None


# log_lines å°‡åœ¨ translate_ui ä¸­ä½œç‚ºå±€éƒ¨è®Šæ•¸åˆå§‹åŒ–


# -----------------------------
# Main Translate UI (New Pipeline: Run Aâ†’E)
# -----------------------------
def translate_ui(
    video_file,
    srt_file,
    srt_encoding,
    enable_vision,
    enable_context_expansion,
    max_frames,
    offsets_csv,
    run_mode: str = "all",  # "all", "A", "B", "C", "D", "E", "F"
    run_e_scheme: str = "full",  # "full" | "main_led" | "local_led" | "draft_first"
    progress=gr.Progress(),
):
    """
    New Pipeline: Run Aâ†’Bâ†’(C/D å¯é¸)â†’E ä¸Šä¸‹æ–‡æ“´å……â†’F ç¿»è­¯ with SubtitleItem and sub_id alignment.
    Run A: Audio; B: Brief (brief.jsonl); C/D: vision æ›´æ–° briefï¼›E: ä¸Šä¸‹æ–‡æ“´å……ï¼›F: æœ€çµ‚ç¿»è­¯ã€‚
    """
    global log_lines
    log_lines = []  # é‡ç½® log
    
    # Guard EVERYTHING so Gradio doesn't just show a red "Error" without details.
    try:
        cfg = _ensure_config()
        cfg.vision.enabled = bool(enable_vision)
        cfg.pipeline.enable_context_expansion = bool(enable_context_expansion)
        cfg.vision.max_frames_per_sub = int(max_frames)
        cfg.pipeline.run_e_scheme = str(run_e_scheme or "full").strip() or "full"

        # ç›®æ¨™èªè¨€ï¼ˆLocale Codeï¼‰ï¼Œå®Œå…¨ç”± language_config.json æ§åˆ¶
        target_locale = _TARGET_LANG_LOCALE.strip() if _TARGET_LANG_LOCALE else "zh-TW"

        # Parse frame offsets (comma-separated 0..1).
        try:
            offsets = [float(x.strip()) for x in str(offsets_csv).split(",") if x.strip()]
            if offsets:
                cfg.vision.frame_offsets = offsets
        except Exception:
            pass
    except Exception as e:
        tb = traceback.format_exc()
        errmsg = _("Error: {error}").format(error=str(e))
        yield None, None, tb[-12000:], None, errmsg, (tb.strip().split("\n")[-1].strip() if tb.strip() else errmsg)
        return

    try:
        # Validate uploads
        srt_path = _gradio_file_path(srt_file)
        video_path = _gradio_video_path(video_file)

        if not srt_path:
            msg = _("Please upload an SRT file.")
            yield None, None, "", None, msg, msg
            return
        
        # Video is required for Run A (audio) and Run C/D (vision)
        if not video_path:
            msg = _("Video is required for audio and vision analysis. Upload a video (MKV/MP4).")
            yield None, None, "", None, msg, msg
            return

        # Validate models exist
        missing = _check_models(cfg)
        if missing:
            msg = (
                _("Missing model files. Please follow the README to download GGUF models into ./models\n\n")
                + "\n".join(missing)
            )
            msg2 = _("Missing models.")
            yield None, None, msg, None, msg2, msg2
            return

        # Show initial status so userçœ‹åˆ°ç¨‹å¼æœ‰é–‹å§‹å‹•ä½œ
        log_lines.append("[Init] Loading configuration and validating inputs...")
        progress(0.01, desc=_("Initializing..."))
        yield None, None, "\n".join(log_lines), None, _("Validating inputs..."), (log_lines[-1] if log_lines else "")

        # Load glossary
        glossary = load_glossary(cfg.glossary.json_path)

        # Load SRT
        subs = pysrt.open(srt_path, encoding=srt_encoding)
        total = len(subs)
        if total == 0:
            msg = _("SRT has 0 lines.")
            yield None, None, "", None, msg, msg
            return

        # å‰µå»º SubtitleItem å­—å…¸ï¼ˆä½¿ç”¨ sub_id å°é½Šï¼‰
        items = create_subtitle_items_from_srt(subs)
        log_lines.append(f"[Init] Created {len(items)} subtitle items with sub_id alignment")
        progress(0.03, desc=_("Parsing subtitles..."))
        yield None, None, "\n".join(log_lines), None, _("Parsing subtitles... ({total} lines)").format(total=total), (log_lines[-1] if log_lines else "")

        # Work directory
        work_dir = Path(cfg.pipeline.work_dir)
        work_dir.mkdir(parents=True, exist_ok=True)
        log_lines.append(f"[Init] Work directory: {work_dir}")

        # Copy video to work_dir so Run A/C/D read a stable path (Gradio temp can cause "Permission denied" in background thread/ffmpeg)
        try:
            import shutil
            src_video = Path(video_path)
            ext = src_video.suffix or ".mkv"
            stable_video_path = work_dir / f"source_video{ext}"
            if str(stable_video_path) != str(Path(video_path).resolve()):
                log_lines.append(f"[Init] Copying video to work dir for stable access: {stable_video_path.name}")
                shutil.copy2(video_path, stable_video_path)
                video_path = str(stable_video_path)
        except Exception as e:
            log_lines.append(f"[Init] âš ï¸ Could not copy video to work dir: {e}; using original path (may cause Permission denied in Run A)")

        # Output pathï¼ˆä¾ target_locale å‘½åï¼Œé¿å…å¯«æ­» zh-TWï¼‰
        safe_locale = target_locale.replace("/", "_").replace("\\", "_")
        out_path = Path(tempfile.gettempdir()) / f"{Path(srt_path).stem}.translated.{safe_locale}.srt"
        preview = None
        preview_holder = [None]  # ä¾› Run C/D å¯«å…¥æœ€å¾Œæ“·å–çš„å½±æ ¼ï¼Œä¾› UI é è¦½

        # è³‡æºæª¢æ¸¬å’Œå‹•æ…‹èª¿æ•´ï¼ˆå„ªå…ˆæª¢æ¸¬ GPUï¼‰
        log_lines.append("[Init] Detecting system resources (GPU/CPU/RAM)...")
        progress(0.05, desc=_("Detecting resources..."))
        yield None, None, "\n".join(log_lines), None, _("Detecting GPU and system resources..."), (log_lines[-1] if log_lines else "")
        
        from src.resource_utils import (
            get_resource_info,
            detect_gpu,
            calculate_batch_size,
            calculate_parallel_workers,
        )
        
        log_lines.append("[Init] Detecting GPU...")
        gpu_info = detect_gpu()
        log_lines.append("[Init] Detecting CPU and memory...")
        resource_info = get_resource_info()
        
        # è¨˜éŒ„è³‡æºè³‡è¨Šï¼ˆå„ªå…ˆé¡¯ç¤º GPUï¼‰
        if gpu_info["available"]:
            gpu_msg = f"[GPU] âœ“ Available - {gpu_info['device_name'] or 'NVIDIA GPU'}"
            if gpu_info["vram_mb"]:
                gpu_msg += f" ({gpu_info['vram_mb']} MB VRAM)"
            log_lines.append(gpu_msg)
            if gpu_info["driver_version"]:
                log_lines.append(f"[GPU] Driver version: {gpu_info['driver_version']}")
        else:
            log_lines.append("[GPU] âœ— Not available - Using CPU mode")
        
        cpu_msg = f"[CPU] Cores: {resource_info['cpu_count']}"
        if resource_info['available_memory_mb']:
            cpu_msg += f", Available RAM: {resource_info['available_memory_mb']} MB"
        log_lines.append(cpu_msg)
        
        # è¨ˆç®—æ‰¹æ¬¡å¤§å°ï¼ˆå¦‚æœæœªåœ¨é…ç½®ä¸­æŒ‡å®šï¼‰
        # å„ªå…ˆä½¿ç”¨ GPU VRAMï¼Œå¦‚æœ GPU ä¸å¯ç”¨å‰‡ä½¿ç”¨ CPU + RAM
        # æ¿€é€²æ¨¡å¼ï¼šä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°ä»¥æé«˜ GPU åˆ©ç”¨ç‡
        if cfg.pipeline.batch_size is None:
            # æ ¹æ“š GPU å¯ç”¨æ€§å’Œç¸½é …ç›®æ•¸å‹•æ…‹èª¿æ•´åŸºç¤æ‰¹æ¬¡å¤§å°
            if gpu_info["available"]:
                # GPU æ¨¡å¼ï¼šä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡ï¼ˆå……åˆ†åˆ©ç”¨ VRAMï¼‰
                if total >= 1000:
                    base_batch_size = 128  # å¤§æª”æ¡ˆï¼šå¤§æ‰¹æ¬¡
                elif total >= 500:
                    base_batch_size = 64   # ä¸­æª”æ¡ˆï¼šä¸­æ‰¹æ¬¡
                else:
                    base_batch_size = 48   # å°æª”æ¡ˆï¼šä¸­æ‰¹æ¬¡
            else:
                # CPU æ¨¡å¼ï¼šä½¿ç”¨è¼ƒå°çš„æ‰¹æ¬¡ï¼ˆé¿å… OOMï¼‰
                base_batch_size = 32
            
            batch_size = calculate_batch_size(
                total_items=total,
                available_memory_mb=resource_info['available_memory_mb'],
                cpu_count=resource_info['cpu_count'],
                gpu_info=gpu_info,
                base_batch_size=base_batch_size,
                min_batch_size=cfg.pipeline.min_batch_size,
                max_batch_size=cfg.pipeline.max_batch_size,
            )
            acceleration_mode = "GPU" if gpu_info["available"] else "CPU"
            log_lines.append(f"[Optimization] Auto-detected batch size: {batch_size} (mode: {acceleration_mode}, base: {base_batch_size})")
        else:
            batch_size = cfg.pipeline.batch_size
            acceleration_mode = "GPU" if gpu_info["available"] else "CPU"
            log_lines.append(f"[Optimization] Using configured batch size: {batch_size} (mode: {acceleration_mode})")
        
        # è¨ˆç®—ä¸¦è¡Œå·¥ä½œæ•¸ï¼ˆå¦‚æœæœªåœ¨é…ç½®ä¸­æŒ‡å®šï¼‰
        if cfg.pipeline.max_workers is None:
            max_workers = calculate_parallel_workers(
                cpu_count=resource_info['cpu_count'],
                max_workers=None,
            )
            log_lines.append(f"[Optimization] Auto-detected max workers: {max_workers}")
        else:
            max_workers = cfg.pipeline.max_workers
        
        # è¨˜éŒ„ GPU åŠ é€Ÿç‹€æ…‹
        if gpu_info["available"]:
            log_lines.append(f"[GPU] Models will be loaded with GPU acceleration (n_gpu_layers=-1)")
        else:
            log_lines.append(f"[GPU] GPU not available - Models will use CPU (slower but still functional)")
        
        progress(0.08, desc=_("Loading prompt configurations..."))
        yield None, None, "\n".join(log_lines), None, _("Loading prompt configurations..."), (log_lines[-1] if log_lines else "")

        # è¼‰å…¥ prompt è¨­å®š
        log_lines.append("[Init] Loading prompt configurations from CSV...")
        reason_path = _resolve_reason_model_path(cfg)
        translate_path = _resolve_translate_model_path(cfg)
        log_lines.append(f"[Init] Reason model: {reason_path.name}")
        log_lines.append(f"[Init] Translate model: {translate_path.name}")
        reason_prompt_config = _load_prompt_from_csv_by_model_name(
            PROMPTS_CSV_PATH, reason_path.name, role="main"
        )
        # Run F ç›¸é—œ prompt å„ªå…ˆå¾ model_prompts_run_e.csv è¼‰å…¥
        reason_assemble_prompt_config = _load_run_e_prompt_from_csv(reason_path.name, "main_assemble")
        translate_prompt_config = _load_run_e_prompt_from_csv(translate_path.name, "localization")
        reason_group_translate_prompt_config = _load_run_e_prompt_from_csv(reason_path.name, "main_group_translate")
        translate_polish_prompt_config = _load_run_e_prompt_from_csv(translate_path.name, "local_polish")
        if RUN_E_PROMPTS_CSV_PATH.exists():
            log_lines.append("[Init] Run F prompts from model_prompts_run_e.csv (fallback: model_prompts.csv)")
        log_lines.append("[Init] Prompt configurations loaded")
        
        progress(0.10, desc=_("Preparing audio analysis (Run A)..."))
        yield None, None, "\n".join(log_lines), None, _("Preparing audio analysis (Run A)..."), (log_lines[-1] if log_lines else "")

        # Progress weights: Audio 20% / Brief 35% / Vision 25% / Translate 20%
        total_progress = 100.0
        progress_audio = 20.0
        progress_brief = 35.0
        progress_vision = 25.0
        progress_expansion = 5.0   # Run E ä¸Šä¸‹æ–‡æ“´å……
        progress_translate = 15.0  # Run F æœ€çµ‚ç¿»è­¯
        current_progress = 0.0

        def update_progress(step_progress: float, desc: str):
            """æ›´æ–°é€²åº¦æ¢"""
            nonlocal current_progress
            current_progress = step_progress
            progress(current_progress / total_progress, desc=desc)

        # ========== Run A: Audio Analysis ==========
        if run_mode in ("all", "A"):
            log_lines.append("[Run A] ====== Starting audio analysis ======")
            yield None, None, "\n".join(log_lines), preview, _("Run A: Audio analysis..."), (log_lines[-1] if log_lines else "")
            update_progress(0.0, _("Run A: Audio analysis"))
            
            # å˜—è©¦è¼‰å…¥å·²å­˜åœ¨çš„çµæœï¼ˆç›¸å®¹èˆŠæ ¼å¼ï¼‰
            try:
                log_lines.append("[Run A] Checking for existing audio analysis results...")
                loaded_items = load_audio_results_compat(work_dir, items)
                if loaded_items and len(loaded_items) == len(items):
                    log_lines.append(f"[Run A] âœ“ Loaded existing audio analysis results: {len(loaded_items)} items")
                    items = loaded_items
                    yield None, None, "\n".join(log_lines), preview, _("Run A: Loaded existing results ({count} items)").format(count=len(items)), (log_lines[-1] if log_lines else "")
                else:
                    log_lines.append("[Run A] No existing results found, starting analysis...")
                    log_lines.append(f"[Run A] Expected to process {len(items)} subtitle items, this may take some time (4K video)...")
                    log_lines.append("[Run A] Note: Each item requires audio segment extraction and analysis, please wait patiently...")
                    yield None, None, "\n".join(log_lines), preview, _("Run A: Starting audio analysis..."), (log_lines[-1] if log_lines else "")
                    
                    # åœ¨å¾Œå°åŸ·è¡Œ run_audioï¼Œä¸¦å®šæœŸ yield ä»¥æ›´æ–° UI
                    # ä½¿ç”¨åŸ·è¡Œç·’åŸ·è¡Œ run_audio
                    result_container = {"items": None, "done": False, "error": None}
                    
                    def run_audio_thread():
                        try:
                            result_container["items"] = run_audio(
                                items,
                                str(video_path),
                                work_dir,
                                cfg,
                                log_lines=log_lines,
                                progress_callback=lambda p, d: update_progress(
                                    (p * progress_audio) / total_progress, d
                                ),
                            )
                            result_container["done"] = True
                        except Exception as e:
                            result_container["error"] = e
                            result_container["done"] = True
                    
                    thread = threading.Thread(target=run_audio_thread, daemon=True)
                    thread.start()
                    
                    # å®šæœŸæª¢æŸ¥é€²åº¦ä¸¦ yield UI æ›´æ–°ï¼ˆåŒæ­¥ç‰ˆï¼‰
                    while not result_container["done"]:
                        time.sleep(1.0)  # æ¯ 1 ç§’æª¢æŸ¥ä¸€æ¬¡
                        yield None, None, "\n".join(log_lines), preview, _("Run A: Processing... (check log for detailed progress)"), (log_lines[-1] if log_lines else "")
                    
                    # ç­‰å¾…åŸ·è¡Œç·’å®Œæˆ
                    thread.join(timeout=1.0)
                    
                    if result_container["error"]:
                        raise result_container["error"]
                    
                    items = result_container["items"]
                    
                    # åŸ·è¡Œå®Œæˆå¾Œç«‹å³ yield ä¸€æ¬¡ä»¥é¡¯ç¤ºæœ€çµ‚ç‹€æ…‹
                    yield None, None, "\n".join(log_lines), preview, _("Run A: Completed ({count} items)").format(count=len(items)), (log_lines[-1] if log_lines else "")
            except Exception as e:
                log_lines.append(f"[Run A] Error loading existing results: {e}, re-analyzing...")
                yield None, None, "\n".join(log_lines), preview, _("Run A: Error occurred, re-analyzing..."), (log_lines[-1] if log_lines else "")
                
                # åŒæ¨£ä½¿ç”¨åŸ·è¡Œç·’åŸ·è¡Œ
                result_container = {"items": None, "done": False, "error": None}
                
                def run_audio_thread():
                    try:
                        result_container["items"] = run_audio(
                            items,
                            str(video_path),
                            work_dir,
                            cfg,
                            log_lines=log_lines,
                            progress_callback=lambda p, d: update_progress(
                                (p * progress_audio) / total_progress, d
                            ),
                        )
                        result_container["done"] = True
                    except Exception as e:
                        result_container["error"] = e
                        result_container["done"] = True
                
                thread = threading.Thread(target=run_audio_thread, daemon=True)
                thread.start()
                
                # å®šæœŸæª¢æŸ¥é€²åº¦ä¸¦ yield UI æ›´æ–°ï¼ˆåŒæ­¥ç‰ˆï¼‰
                while not result_container["done"]:
                    time.sleep(1.0)
                    yield None, None, "\n".join(log_lines), preview, _("Run A: Processing... (check log for detailed progress)"), (log_lines[-1] if log_lines else "")
                
                thread.join(timeout=1.0)
                
                if result_container["error"]:
                    raise result_container["error"]
                
                items = result_container["items"]
            
            update_progress(progress_audio, _("Run A: Completed"))
            yield None, None, "\n".join(log_lines), preview, _("Run A: Completed ({count} items)").format(count=len(items)), (log_lines[-1] if log_lines else "")
        
        # ========== Run B: Brief Generation v1 ==========
        if run_mode in ("all", "B"):
            log_lines.append("[Run B] Preparing to load main reasoning model (Stage 2) and generate brief_v1...")
            yield None, None, "\n".join(log_lines), preview, _("Run B: Loading main reasoning model (Stage 2)..."), (log_lines[-1] if log_lines else "")
            update_progress(progress_audio, _("Run B: Loading main reasoning model (Stage 2)"))
            
            # å˜—è©¦è¼‰å…¥å·²å­˜åœ¨çš„çµæœï¼ˆç›¸å®¹èˆŠæ ¼å¼ï¼‰
            try:
                loaded_items = load_brief_results_compat(work_dir, "v1", items)
                if loaded_items and len(loaded_items) == len(items) and _cache_pack_ok(loaded_items, "v1"):
                    log_lines.append(f"[Run B] Loaded existing brief v1: {len(loaded_items)} items")
                    items = loaded_items
                else:
                    if loaded_items and len(loaded_items) == len(items) and not _cache_pack_ok(loaded_items, "v1"):
                        log_lines.append("[Run B] Cache missing PACK, forcing regenerate")
                    else:
                        log_lines.append("[Run B] No existing brief v1 or count mismatch, loading main model and generating...")
                    yield None, None, "\n".join(log_lines), preview, _("Run B: Loading main reasoning model (Stage 2)..."), (log_lines[-1] if log_lines else "")
                    result_container = {"items": None, "done": False, "error": None}
                    def run_brief_b():
                        try:
                            result_container["items"] = run_brief_text(
                                items,
                                work_dir,
                                cfg,
                                reason_prompt_config,
                                version="v1",
                                vision_hint_map=None,
                                log_lines=log_lines,
                                progress_callback=lambda p, d: update_progress(
                                    (progress_audio + p * progress_brief) / total_progress, d
                                ),
                                batch_size=batch_size if batch_size < total else None,
                                target_language=target_locale,
                            )
                        except Exception as e:
                            result_container["error"] = e
                        result_container["done"] = True
                    thread = threading.Thread(target=run_brief_b, daemon=True)
                    thread.start()
                    while not result_container["done"]:
                        time.sleep(1.0)
                        yield None, None, "\n".join(log_lines), preview, _("Run B: Loading model / processing... (see log)"), (log_lines[-1] if log_lines else "")
                    thread.join(timeout=1.0)
                    if result_container["error"]:
                        raise result_container["error"]
                    items = result_container["items"]
                    yield None, None, "\n".join(log_lines), preview, _("Run B: Model loaded, processing with parallel batch inference..."), (log_lines[-1] if log_lines else "")
            except Exception as e:
                log_lines.append(f"[Run B] Error loading existing results: {e}, reloading main model and regenerating...")
                yield None, None, "\n".join(log_lines), preview, _("Run B: Reloading main reasoning model (Stage 2)..."), (log_lines[-1] if log_lines else "")
                result_container = {"items": None, "done": False, "error": None}
                def run_brief_b():
                    try:
                        result_container["items"] = run_brief_text(
                            items,
                            work_dir,
                            cfg,
                            reason_prompt_config,
                            version="v1",
                            vision_hint_map=None,
                            log_lines=log_lines,
                            progress_callback=lambda p, d: update_progress(
                                (progress_audio + p * progress_brief) / total_progress, d
                            ),
                            batch_size=batch_size if batch_size < total else None,
                            target_language=target_locale,
                        )
                    except Exception as ex:
                        result_container["error"] = ex
                    result_container["done"] = True
                thread = threading.Thread(target=run_brief_b, daemon=True)
                thread.start()
                while not result_container["done"]:
                    time.sleep(1.0)
                    yield None, None, "\n".join(log_lines), preview, _("Run B: Reloading / processing... (see log)"), (log_lines[-1] if log_lines else "")
                thread.join(timeout=1.0)
                if result_container["error"]:
                    raise result_container["error"]
                items = result_container["items"]
                yield None, None, "\n".join(log_lines), preview, _("Run B: Model reloaded, processing with parallel batch inference..."), (log_lines[-1] if log_lines else "")
            
            # Debug logï¼šé¡¯ç¤ºå‰ 3 å¥çš„ audio_hintï¼ˆç”¨æ–¼é©—æ”¶ï¼‰
            sorted_items = sorted(items.items(), key=lambda x: x[1].start_ms)
            for debug_idx in range(min(3, len(sorted_items))):
                sub_id, item = sorted_items[debug_idx]
                audio_hint = item.get_audio_hint()
                log_lines.append(f"[Debug] Item {debug_idx+1} (sub_id={sub_id[:8]}...) audio_hint: {audio_hint}")
            
            update_progress(progress_audio + progress_brief, _("Run B: Completed"))
            yield None, None, "\n".join(log_lines), preview, _("Run B: Completed ({count} items)").format(count=len(items)), (log_lines[-1] if log_lines else "")
        
        # ========== Run C: Single-frame Vision Fallback ==========
        # æª¢æŸ¥ vision æ¨¡å‹æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if cfg.vision.enabled:
            vision_ok, vision_missing = _check_vision_assets(cfg)
            if not vision_ok:
                # è‡ªå‹•åœç”¨ visionï¼ˆåƒ… runtimeï¼Œä¸ä¿®æ”¹ config.jsonï¼‰
                log_lines.append(f"[Vision] âš ï¸ Vision disabled: missing files:")
                for missing_file in vision_missing:
                    log_lines.append(f"[Vision]   - {missing_file}")
                log_lines.append(f"[Vision] Please place vision model files in: {cfg.models_dir}")
                cfg.vision.enabled = False  # Runtime disable
        
        if run_mode in ("all", "C") and cfg.vision.enabled:
            # ç¢ºä¿æœ‰ brief_v1ï¼ˆC-only æ™‚å¾ brief.jsonl / brief_v1.jsonl è¼‰å…¥ï¼‰
            loaded_brief = load_brief_results_compat(work_dir, "v1", items)
            if loaded_brief and len(loaded_brief) == len(items):
                items = loaded_brief
            # æ‰¾å‡ºéœ€è¦ vision çš„ sub_idï¼ˆbrief_v1.need_vision === trueï¼‰
            sorted_items = sorted(items.items(), key=lambda x: x[1].start_ms)
            target_sub_ids = [
                sub_id for sub_id, item in sorted_items
                if item.brief_v1 and item.brief_v1.need_vision is True
            ]

            if target_sub_ids:
                log_lines.append(f"[Run C] Preparing to load vision model (Stage 3, 1-frame) and analyze {len(target_sub_ids)} items...")
                yield None, None, "\n".join(log_lines), preview, _("Run C: Loading vision model (1-frame) for {count} items...").format(count=len(target_sub_ids)), (log_lines[-1] if log_lines else "")
                update_progress(progress_audio + progress_brief, _("Run C: Loading vision model (1-frame)"))
                
                # å˜—è©¦è¼‰å…¥å·²å­˜åœ¨çš„è¦–è¦ºçµæœï¼ˆç›¸å®¹èˆŠæ ¼å¼ï¼‰
                try:
                    loaded_items = load_vision_results_compat(work_dir, items, single_frame=True)
                    if loaded_items and len(loaded_items) == len(items):
                        log_lines.append(f"[Run C] Loaded existing vision (1-frame) results: {len(loaded_items)} items")
                        items = loaded_items
                    else:
                        log_lines.append("[Run C] No existing vision (1-frame) results or count mismatch, loading vision model...")
                        yield None, None, "\n".join(log_lines), preview, _("Run C: Loading vision model (1-frame)..."), (log_lines[-1] if log_lines else "")
                        items = run_vision_single(
                            items,
                            str(video_path),
                            work_dir,
                            cfg,
                            target_sub_ids,
                            log_lines=log_lines,
                            progress_callback=lambda p, d: update_progress(
                                (progress_audio + progress_brief + p * (progress_vision * 0.5)) / total_progress, d
                            ),
                            preview_callback=lambda b: preview_holder.__setitem__(0, _save_preview_bytes_to_file(b, work_dir)),
                        )
                        if preview_holder[0] is not None:
                            preview = preview_holder[0]
                            if isinstance(preview, bytes):
                                preview = _save_preview_bytes_to_file(preview, work_dir)
                        yield None, None, "\n".join(log_lines), preview, _("Run C: Vision model loaded, analyzing frames..."), (log_lines[-1] if log_lines else "")
                except Exception as e:
                    log_lines.append(f"[Run C] Error loading existing results: {e}, reloading vision model...")
                    yield None, None, "\n".join(log_lines), preview, _("Run C: Reloading vision model (1-frame)..."), (log_lines[-1] if log_lines else "")
                    items = run_vision_single(
                        items,
                        str(video_path),
                        work_dir,
                        cfg,
                        target_sub_ids,
                        log_lines=log_lines,
                        progress_callback=lambda p, d: update_progress(
                            (progress_audio + progress_brief + p * (progress_vision * 0.5)) / total_progress, d
                        ),
                        preview_callback=lambda b: preview_holder.__setitem__(0, _save_preview_bytes_to_file(b, work_dir)),
                    )
                    if preview_holder[0] is not None:
                        preview = preview_holder[0]
                        if isinstance(preview, bytes):
                            preview = _save_preview_bytes_to_file(preview, work_dir)
                    yield None, None, "\n".join(log_lines), preview, _("Run C: Vision model reloaded, analyzing frames..."), (log_lines[-1] if log_lines else "")
                
                # é‡æ–°ç”Ÿæˆ brief v2ï¼ˆå¸¶è¦–è¦ºæç¤ºï¼‰ï¼›æ›´æ–°å‰å…ˆç•™ snapshot
                _copy_brief_snapshot(work_dir, "v1")
                yield None, None, "\n".join(log_lines), preview, _("Run C: Regenerating brief v2 with vision..."), (log_lines[-1] if log_lines else "")

                # å»ºç«‹ vision_hint_map
                vision_hint_map = {
                    sub_id: item.vision_desc_1
                    for sub_id, item in items.items()
                    if item.vision_desc_1
                }
                
                # å˜—è©¦è¼‰å…¥å·²å­˜åœ¨çš„ brief_v2
                try:
                    loaded_items = load_brief_results_compat(work_dir, "v2", items)
                    if loaded_items and len(loaded_items) == len(items) and _cache_pack_ok(loaded_items, "v2"):
                        log_lines.append(f"[Run C] Loaded existing brief v2: {len(loaded_items)} items")
                        items = loaded_items
                    else:
                        if loaded_items and len(loaded_items) == len(items) and not _cache_pack_ok(loaded_items, "v2"):
                            log_lines.append("[Run C] Cache missing PACK, forcing regenerate")
                        else:
                            log_lines.append("[Run C] No existing brief v2 or count mismatch, reloading main model and regenerating...")
                        yield None, None, "\n".join(log_lines), preview, _("Run C: Reloading main reasoning model for brief v2..."), (log_lines[-1] if log_lines else "")
                        result_container = {"items": None, "done": False, "error": None}
                        def run_brief_c2():
                            try:
                                result_container["items"] = run_brief_text(
                                    items,
                                    work_dir,
                                    cfg,
                                    reason_prompt_config,
                                    version="v2",
                                    vision_hint_map=vision_hint_map,
                                    log_lines=log_lines,
                                    progress_callback=lambda p, d: update_progress(
                                        (progress_audio + progress_brief + p * (progress_vision * 0.5)) / total_progress, d
                                    ),
                                    batch_size=batch_size if batch_size < total else None,
                                    target_language=target_locale,
                                )
                            except Exception as ex:
                                result_container["error"] = ex
                            result_container["done"] = True
                        thread = threading.Thread(target=run_brief_c2, daemon=True)
                        thread.start()
                        while not result_container["done"]:
                            time.sleep(1.0)
                            yield None, None, "\n".join(log_lines), preview, _("Run C: Loading model / brief v2... (see log)"), (log_lines[-1] if log_lines else "")
                        thread.join(timeout=1.0)
                        if result_container["error"]:
                            raise result_container["error"]
                        items = result_container["items"]
                        yield None, None, "\n".join(log_lines), preview, _("Run C: Main model reloaded, regenerating brief v2 with parallel batch inference..."), (log_lines[-1] if log_lines else "")
                except Exception as e:
                    log_lines.append(f"[Run C] Error loading existing brief v2: {e}, reloading main model and regenerating...")
                    yield None, None, "\n".join(log_lines), preview, _("Run C: Reloading main reasoning model for brief v2..."), (log_lines[-1] if log_lines else "")
                    result_container = {"items": None, "done": False, "error": None}
                    def run_brief_c2():
                        try:
                            result_container["items"] = run_brief_text(
                                items,
                                work_dir,
                                cfg,
                                reason_prompt_config,
                                version="v2",
                                vision_hint_map=vision_hint_map,
                                log_lines=log_lines,
                                progress_callback=lambda p, d: update_progress(
                                    (progress_audio + progress_brief + p * (progress_vision * 0.5)) / total_progress, d
                                ),
                                batch_size=batch_size if batch_size < total else None,
                                target_language=target_locale,
                            )
                        except Exception as ex:
                            result_container["error"] = ex
                        result_container["done"] = True
                    thread = threading.Thread(target=run_brief_c2, daemon=True)
                    thread.start()
                    while not result_container["done"]:
                        time.sleep(1.0)
                        yield None, None, "\n".join(log_lines), preview, _("Run C: Reloading / brief v2... (see log)"), (log_lines[-1] if log_lines else "")
                    thread.join(timeout=1.0)
                    if result_container["error"]:
                        raise result_container["error"]
                    items = result_container["items"]
                    yield None, None, "\n".join(log_lines), preview, _("Run C: Main model reloaded, regenerating brief v2 with parallel batch inference..."), (log_lines[-1] if log_lines else "")
            else:
                log_lines.append("[Run C] No items need vision (1-frame) fallback")
            
            update_progress(progress_audio + progress_brief + progress_vision * 0.5, _("Run C: Completed"))
            yield None, None, "\n".join(log_lines), preview, _("Run C: Completed ({count} items)").format(count=len(items)), (log_lines[-1] if log_lines else "")
            # ç„¡è«–æ˜¯å¦åŸ·è¡Œ Run Dï¼ŒRun C çµæŸå¾Œéƒ½å»ºç«‹ brief_v2 å¿«ç…§ï¼ˆä»£è¡¨ã€ŒRun C å¾Œçš„ brief ç‹€æ…‹ã€ï¼‰
            _copy_brief_snapshot(work_dir, "v2")
        
        # ========== Run D: Multi-frame Vision Fallback ==========
        # æª¢æŸ¥ vision æ¨¡å‹æª”æ¡ˆæ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœä¹‹å‰æœªæª¢æŸ¥éï¼‰
        if cfg.vision.enabled:
            vision_ok, vision_missing = _check_vision_assets(cfg)
            if not vision_ok:
                # è‡ªå‹•åœç”¨ visionï¼ˆåƒ… runtimeï¼Œä¸ä¿®æ”¹ config.jsonï¼‰
                if "[Vision] âš ï¸ Vision disabled" not in "\n".join(log_lines[-50:]):  # é¿å…é‡è¤‡æç¤º
                    log_lines.append(f"[Vision] âš ï¸ Vision disabled: missing files:")
                    for missing_file in vision_missing:
                        log_lines.append(f"[Vision]   - {missing_file}")
                    log_lines.append(f"[Vision] Please place vision model files in: {cfg.models_dir}")
                cfg.vision.enabled = False  # Runtime disable
        
        if run_mode in ("all", "D") and cfg.vision.enabled:
            # æ‰¾å‡ºéœ€è¦å¤šå¼µå½±åƒçš„ sub_idï¼ˆbrief_v2.need_multi_frame_vision === trueï¼Œè‹¥ç„¡å‰‡ç”¨ brief_v1ï¼‰
            sorted_items = sorted(items.items(), key=lambda x: x[1].start_ms)
            target_sub_ids = []
            for sub_id, item in sorted_items:
                brief_to_check = item.brief_v2 if item.brief_v2 else item.brief_v1
                if brief_to_check and brief_to_check.need_multi_frame_vision is True:
                    target_sub_ids.append(sub_id)
            
            if target_sub_ids:
                log_lines.append(f"[Run D] Preparing to load vision model (Stage 3, multi-frame) and analyze {len(target_sub_ids)} items...")
                yield None, None, "\n".join(log_lines), preview, _("Run D: Loading vision model (multi-frame) for {count} items...").format(count=len(target_sub_ids)), (log_lines[-1] if log_lines else "")
                update_progress(progress_audio + progress_brief + progress_vision * 0.5, _("Run D: Loading vision model (multi-frame)"))
                
                # å˜—è©¦è¼‰å…¥å·²å­˜åœ¨çš„è¦–è¦ºçµæœï¼ˆç›¸å®¹èˆŠæ ¼å¼ï¼‰
                try:
                    loaded_items = load_vision_results_compat(work_dir, items, single_frame=False)
                    if loaded_items and len(loaded_items) == len(items):
                        log_lines.append(f"[Run D] Loaded existing vision (multi-frame) results: {len(loaded_items)} items")
                        items = loaded_items
                    else:
                        log_lines.append("[Run D] No existing vision (multi-frame) results or count mismatch, loading vision model...")
                        yield None, None, "\n".join(log_lines), preview, _("Run D: Loading vision model (multi-frame)..."), (log_lines[-1] if log_lines else "")
                        items = run_vision_multi(
                            items,
                            str(video_path),
                            work_dir,
                            cfg,
                            target_sub_ids,
                            cfg.vision.max_frames_per_sub,
                            log_lines=log_lines,
                            progress_callback=lambda p, d: update_progress(
                                (progress_audio + progress_brief + progress_vision * 0.5 + p * (progress_vision * 0.5)) / total_progress, d
                            ),
                            preview_callback=lambda b: preview_holder.__setitem__(0, _save_preview_bytes_to_file(b, work_dir)),
                        )
                        if preview_holder[0] is not None:
                            preview = preview_holder[0]
                            if isinstance(preview, bytes):
                                preview = _save_preview_bytes_to_file(preview, work_dir)
                        yield None, None, "\n".join(log_lines), preview, _("Run D: Vision model loaded, analyzing multiple frames..."), (log_lines[-1] if log_lines else "")
                except Exception as e:
                    log_lines.append(f"[Run D] Error loading existing results: {e}, reloading vision model...")
                    yield None, None, "\n".join(log_lines), preview, _("Run D: Reloading vision model (multi-frame)..."), (log_lines[-1] if log_lines else "")
                    items = run_vision_multi(
                        items,
                        str(video_path),
                        work_dir,
                        cfg,
                        target_sub_ids,
                        cfg.vision.max_frames_per_sub,
                        log_lines=log_lines,
                        progress_callback=lambda p, d: update_progress(
                            (progress_audio + progress_brief + progress_vision * 0.5 + p * (progress_vision * 0.5)) / total_progress, d
                        ),
                        preview_callback=lambda b: preview_holder.__setitem__(0, _save_preview_bytes_to_file(b, work_dir)),
                    )
                    if preview_holder[0] is not None:
                        preview = preview_holder[0]
                        if isinstance(preview, bytes):
                            preview = _save_preview_bytes_to_file(preview, work_dir)
                    yield None, None, "\n".join(log_lines), preview, _("Run D: Vision model reloaded, analyzing multiple frames..."), (log_lines[-1] if log_lines else "")
                
                # é‡æ–°ç”Ÿæˆ brief v3ï¼ˆå¸¶å¤šå¼µå½±åƒæç¤ºï¼‰ï¼›æ›´æ–°å‰å…ˆç•™ snapshot
                _copy_brief_snapshot(work_dir, "v2")
                yield None, None, "\n".join(log_lines), preview, _("Run D: Regenerating brief v3 with multi-frame vision..."), (log_lines[-1] if log_lines else "")

                # å»ºç«‹ vision_hint_map
                vision_hint_map = {
                    sub_id: item.vision_desc_n
                    for sub_id, item in items.items()
                    if item.vision_desc_n
                }
                
                # å˜—è©¦è¼‰å…¥å·²å­˜åœ¨çš„ brief_v3
                try:
                    loaded_items = load_brief_results_compat(work_dir, "v3", items)
                    if loaded_items and len(loaded_items) == len(items) and _cache_pack_ok(loaded_items, "v3"):
                        log_lines.append(f"[Run D] Loaded existing brief v3: {len(loaded_items)} items")
                        items = loaded_items
                    else:
                        if loaded_items and len(loaded_items) == len(items) and not _cache_pack_ok(loaded_items, "v3"):
                            log_lines.append("[Run D] Cache missing PACK, forcing regenerate")
                        else:
                            log_lines.append("[Run D] No existing brief v3 or count mismatch, reloading main model and regenerating...")
                        yield None, None, "\n".join(log_lines), preview, _("Run D: Reloading main reasoning model for brief v3..."), (log_lines[-1] if log_lines else "")
                        result_container = {"items": None, "done": False, "error": None}
                        def run_brief_d3():
                            try:
                                result_container["items"] = run_brief_text(
                                    items,
                                    work_dir,
                                    cfg,
                                    reason_prompt_config,
                                    version="v3",
                                    vision_hint_map=vision_hint_map,
                                    log_lines=log_lines,
                                    progress_callback=lambda p, d: update_progress(
                                        (progress_audio + progress_brief + progress_vision * 0.5 + p * (progress_vision * 0.5)) / total_progress, d
                                    ),
                                    batch_size=batch_size if batch_size < total else None,
                                    target_language=target_locale,
                                )
                            except Exception as ex:
                                result_container["error"] = ex
                            result_container["done"] = True
                        thread = threading.Thread(target=run_brief_d3, daemon=True)
                        thread.start()
                        while not result_container["done"]:
                            time.sleep(1.0)
                            yield None, None, "\n".join(log_lines), preview, _("Run D: Loading model / brief v3... (see log)"), (log_lines[-1] if log_lines else "")
                        thread.join(timeout=1.0)
                        if result_container["error"]:
                            raise result_container["error"]
                        items = result_container["items"]
                        yield None, None, "\n".join(log_lines), preview, _("Run D: Main model reloaded, regenerating brief v3 with parallel batch inference..."), (log_lines[-1] if log_lines else "")
                except Exception as e:
                    log_lines.append(f"[Run D] Error loading existing brief v3: {e}, reloading main model and regenerating...")
                    yield None, None, "\n".join(log_lines), preview, _("Run D: Reloading main reasoning model for brief v3..."), (log_lines[-1] if log_lines else "")
                    result_container = {"items": None, "done": False, "error": None}
                    def run_brief_d3():
                        try:
                            result_container["items"] = run_brief_text(
                                items,
                                work_dir,
                                cfg,
                                reason_prompt_config,
                                version="v3",
                                vision_hint_map=vision_hint_map,
                                log_lines=log_lines,
                                progress_callback=lambda p, d: update_progress(
                                    (progress_audio + progress_brief + progress_vision * 0.5 + p * (progress_vision * 0.5)) / total_progress, d
                                ),
                                batch_size=batch_size if batch_size < total else None,
                                target_language=target_locale,
                            )
                        except Exception as ex:
                            result_container["error"] = ex
                        result_container["done"] = True
                    thread = threading.Thread(target=run_brief_d3, daemon=True)
                    thread.start()
                    while not result_container["done"]:
                        time.sleep(1.0)
                        yield None, None, "\n".join(log_lines), preview, _("Run D: Reloading / brief v3... (see log)"), (log_lines[-1] if log_lines else "")
                    thread.join(timeout=1.0)
                    if result_container["error"]:
                        raise result_container["error"]
                    items = result_container["items"]
                    yield None, None, "\n".join(log_lines), preview, _("Run D: Main model reloaded, regenerating brief v3 with parallel batch inference..."), (log_lines[-1] if log_lines else "")
            else:
                log_lines.append("[Run D] No items need vision (multi-frame) fallback")
            
            update_progress(progress_audio + progress_brief + progress_vision, _("Run D: Completed"))
            yield None, None, "\n".join(log_lines), preview, _("Run D: Completed ({count} items)").format(count=len(items)), (log_lines[-1] if log_lines else "")

        # ========== Run E: Context Expansionï¼ˆneed_more_context ç”¨ prev-3/next-3 æ›´æ–° briefï¼‰==========
        # åƒ…åœ¨ run_mode ç‚º E æˆ–ï¼ˆall ä¸” å•Ÿç”¨æ›´å¤šä¸Šä¸‹æ–‡å‚™æ´ï¼‰æ™‚åŸ·è¡Œ
        if run_mode in ("all", "E") and (run_mode == "E" or getattr(cfg.pipeline, "enable_context_expansion", True)):
            # ç¢ºä¿æœ‰ç•¶å‰ briefï¼ˆE-only æ™‚å¾ brief_work.jsonl / èˆŠå brief.jsonl è¼‰å…¥ï¼‰
            loaded_brief_e = load_brief_results_compat(work_dir, "v3", items)
            if loaded_brief_e and len(loaded_brief_e) == len(items):
                items = loaded_brief_e
            _copy_brief_snapshot(work_dir, "v3")
            log_lines.append("[Run E] Context expansion (need_more_context â†’ prev-3/next-3 stage2)...")
            yield None, None, "\n".join(log_lines), preview, _("Run E: Context expansion..."), (log_lines[-1] if log_lines else "")
            update_progress(progress_audio + progress_brief + progress_vision, _("Run E: Context expansion"))
            items = run_context_expansion(
                items,
                work_dir,
                cfg,
                reason_prompt_config,
                target_language=target_locale,
                log_lines=log_lines,
                progress_callback=lambda p, d: update_progress(
                    (progress_audio + progress_brief + progress_vision + p * progress_expansion) / total_progress, d
                ),
            )
            update_progress(progress_audio + progress_brief + progress_vision + progress_expansion, _("Run E: Completed"))
            yield None, None, "\n".join(log_lines), preview, _("Run E: Completed"), (log_lines[-1] if log_lines else "")

        # ========== Run F: Final Translation ==========
        name_mappings: list = []
        if run_mode in ("all", "F"):
            # Run F å‰å…ˆç•™ä¸€ä»½ snapshotï¼ˆbrief_v4.jsonlï¼‰ï¼Œä¸¦ç¢ºä¿ brief ç‚º E æ›´æ–°å¾Œï¼ˆå¾ brief_work.jsonl / èˆŠå brief.jsonl è¼‰å…¥ï¼‰
            _copy_brief_snapshot(work_dir, "v4")
            loaded_brief_f = load_brief_results_compat(work_dir, "v3", items)
            if loaded_brief_f and len(loaded_brief_f) == len(items):
                items = loaded_brief_f
            log_lines.append("[Run F] Preparing to load translation model (localization) and generate final subtitles...")
            yield None, None, "\n".join(log_lines), preview, _("Run F: Loading translation model (localization)..."), (log_lines[-1] if log_lines else "")
            update_progress(progress_audio + progress_brief + progress_vision + progress_expansion, _("Run F: Loading translation model (localization)"))

            # å˜—è©¦è¼‰å…¥å·²å­˜åœ¨çš„æœ€çµ‚ç¿»è­¯çµæœï¼ˆç›¸å®¹èˆŠæ ¼å¼ï¼‰
            try:
                loaded_items = load_final_translations_compat(work_dir, items)
                if loaded_items and len(loaded_items) == len(items):
                    log_lines.append(f"[Run F] Loaded existing final translations: {len(loaded_items)} items")
                    items = loaded_items
                else:
                    log_lines.append("[Run F] No existing final translations or count mismatch, loading translation model...")
                    yield None, None, "\n".join(log_lines), preview, _("Run F: Loading translation model (localization)..."), (log_lines[-1] if log_lines else "")
                    result_container = {"items": None, "done": False, "error": None}
                    def run_translate_f():
                        try:
                            result_container["items"] = run_final_translate(
                                items,
                                work_dir,
                                cfg,
                                glossary,
                                target_locale,
                                translate_prompt_config,
                                reason_prompt_config=reason_prompt_config,
                                reason_assemble_prompt_config=reason_assemble_prompt_config,
                                reason_group_translate_prompt_config=reason_group_translate_prompt_config,
                                translate_polish_prompt_config=translate_polish_prompt_config,
                                log_lines=log_lines,
                                progress_callback=lambda p, d: update_progress(
                                    (progress_audio + progress_brief + progress_vision + progress_expansion + p * progress_translate) / total_progress, d
                                ),
                                batch_size=batch_size if batch_size < total else None,
                            )
                        except Exception as ex:
                            result_container["error"] = ex
                        result_container["done"] = True
                    thread = threading.Thread(target=run_translate_f, daemon=True)
                    thread.start()
                    while not result_container["done"]:
                        time.sleep(1.0)
                        yield None, None, "\n".join(log_lines), preview, _("Run F: Loading translation model / processing... (see log)"), (log_lines[-1] if log_lines else "")
                    thread.join(timeout=1.0)
                    if result_container["error"]:
                        raise result_container["error"]
                    result = result_container["items"]
                    items = result[0] if isinstance(result, (list, tuple)) and result else result
                    name_mappings = result[1] if isinstance(result, (list, tuple)) and len(result) > 1 else []
                    yield None, None, "\n".join(log_lines), preview, _("Run F: Translation model loaded, processing with parallel batch inference..."), (log_lines[-1] if log_lines else "")
            except Exception as e:
                log_lines.append(f"[Run F] Error loading existing translations: {e}, reloading translation model...")
                yield None, None, "\n".join(log_lines), preview, _("Run F: Reloading translation model (localization)..."), (log_lines[-1] if log_lines else "")
                result_container = {"items": None, "done": False, "error": None}
                def run_translate_f():
                    try:
                        result_container["items"] = run_final_translate(
                            items,
                            work_dir,
                            cfg,
                            glossary,
                            target_locale,
                            translate_prompt_config,
                            reason_prompt_config=reason_prompt_config,
                            reason_assemble_prompt_config=reason_assemble_prompt_config,
                            reason_group_translate_prompt_config=reason_group_translate_prompt_config,
                            translate_polish_prompt_config=translate_polish_prompt_config,
                            log_lines=log_lines,
                            progress_callback=lambda p, d: update_progress(
                                (progress_audio + progress_brief + progress_vision + progress_expansion + p * progress_translate) / total_progress, d
                            ),
                            batch_size=batch_size if batch_size < total else None,
                        )
                    except Exception as ex:
                        result_container["error"] = ex
                    result_container["done"] = True
                thread = threading.Thread(target=run_translate_f, daemon=True)
                thread.start()
                while not result_container["done"]:
                    time.sleep(1.0)
                    yield None, None, "\n".join(log_lines), preview, _("Run F: Reloading / processing... (see log)"), (log_lines[-1] if log_lines else "")
                thread.join(timeout=1.0)
                if result_container["error"]:
                    raise result_container["error"]
                result = result_container["items"]
                items = result[0] if isinstance(result, (list, tuple)) and result else result
                name_mappings = result[1] if isinstance(result, (list, tuple)) and len(result) > 1 else []
                yield None, None, "\n".join(log_lines), preview, _("Run F: Translation model reloaded, processing with parallel batch inference..."), (log_lines[-1] if log_lines else "")
            
            # å°‡ç¿»è­¯çµæœå¯«å› SRTï¼šä¾ (start_ms, end_ms) å°é½Šï¼ŒåŒä¸€æ™‚é–“éµå¯å°æ‡‰å¤šç­†å­—å¹•ï¼Œä¾åŸå§‹é †åºæ¶ˆè€—
            time_to_item: dict[tuple[int, int], list] = {}
            for sub_id in sorted(items.keys()):
                item = items[sub_id]
                key = (round(item.start_ms), round(item.end_ms))
                time_to_item.setdefault(key, []).append(item)
            for sub in subs:
                start_ms, end_ms, _mid = sub_midpoints_ms(sub)
                key = (round(start_ms), round(end_ms))
                queue = time_to_item.get(key)
                if not queue:
                    continue
                item = queue.pop(0)
                text = (item.translated_text or "").strip()
                if not text:
                    continue
                # ä¿ç•™åŸæ–‡çš„ <i> æ¨™ç±¤ï¼šè‹¥åŸæ–‡æœ‰ <i> å‰‡ç”¨ <i> åŒ…ä½è­¯æ–‡
                orig_raw = (sub.text or "").strip()
                if "<i>" in orig_raw or orig_raw.startswith("<i"):
                    text = f"<i>{text}</i>" if "<i>" not in text else text
                sub.text = text

            # ä¿å­˜æœ€çµ‚ SRT
                save_srt(subs, str(out_path), encoding="utf-8")

            # äººå/å°ˆæœ‰åè© CSVï¼ˆæ ¼å¼åŒè¡“èªè¡¨ï¼šEnglish, Target, Note, Enabledï¼‰
            names_csv_path: Optional[Path] = None
            if name_mappings:
                names_csv_path = out_path.parent / f"{out_path.stem}.names.csv"
                target_col = target_locale or "Target"
                with open(names_csv_path, "w", encoding="utf-8-sig", newline="") as f:
                    w = csv.writer(f)
                    w.writerow(["English", target_col, "Note", "Enabled"])
                    for en, target in name_mappings:
                        w.writerow([en, target or "", "", True])
            
            update_progress(total_progress, _("Run F: Completed"))
            yield str(out_path), str(names_csv_path) if names_csv_path else None, "\n".join(log_lines), preview, _("All runs completed!"), (log_lines[-1] if log_lines else "")
        else:
            # Run F æœªåŸ·è¡Œï¼Œä½†å¯èƒ½æœ‰éƒ¨åˆ†çµæœ
            name_mappings = []
            yield None, None, "\n".join(log_lines), preview, _("Translation not executed (Run F not selected)."), (log_lines[-1] if log_lines else "")

    except Exception as e:
        tb = traceback.format_exc()
        # Keep UI usable: show traceback in Log and a short message in Status.
        # ç¢ºä¿å¸è¼‰æ¨¡å‹ï¼ˆæ–° pipeline ä½¿ç”¨ ModelMutexï¼Œæœƒåœ¨ finally ä¸­è‡ªå‹•è™•ç†ï¼‰
        errmsg = _("Error: {error}").format(error=str(e))
        yield None, None, tb[-12000:], None, errmsg, (tb.strip().split("\n")[-1].strip() if tb.strip() else errmsg)
        return


# -----------------------------
# Glossary UI
# -----------------------------
def glossary_load():
    cfg = _ensure_config()
    entries = load_glossary(cfg.glossary.json_path)
    return [[e.en, e.zh, e.note, e.enabled] for e in entries], _("Loaded {count} entries from {path}").format(count=len(entries), path=cfg.glossary.json_path)


def glossary_save(table):
    cfg = _ensure_config()
    entries = []
    
    # Handle Gradio DataFrame - it might be a pandas DataFrame or a list
    if table is None:
        table = []
    elif hasattr(table, 'values'):
        # pandas DataFrame
        table = table.values.tolist()
    elif not isinstance(table, list):
        # Try to convert to list
        try:
            table = list(table)
        except Exception:
            table = []
    
    for row in table:
        if not row:
            continue
        # Handle row as list or tuple
        if not isinstance(row, (list, tuple)):
            continue
        if len(row) < 2:
            continue
        en = (row[0] or "").strip() if row[0] is not None else ""
        zh = (row[1] or "").strip() if row[1] is not None else ""
        if not en or not zh:
            continue
        note = (row[2] or "").strip() if len(row) > 2 and row[2] is not None else ""
        enabled = bool(row[3]) if len(row) > 3 and row[3] is not None else True
        entries.append(GlossaryEntry(en=en, zh=zh, note=note, enabled=enabled))
    save_glossary(cfg.glossary.json_path, entries)
    return _("Saved {count} entries to {path}").format(count=len(entries), path=cfg.glossary.json_path)


def glossary_import_csv(file_obj):
    cfg = _ensure_config()
    if not file_obj:
        return gr.update(), _("No file.")
    data = _read_uploaded_file_bytes(file_obj)
    new_entries = import_csv_two_cols(data)
    entries = load_glossary(cfg.glossary.json_path) + new_entries
    save_glossary(cfg.glossary.json_path, entries)
    return [[e.en, e.zh, e.note, e.enabled] for e in entries], _("Imported {new_count} from CSV. Total {total_count}.").format(new_count=len(new_entries), total_count=len(entries))


def glossary_import_template(file_obj):
    cfg = _ensure_config()
    if not file_obj:
        return gr.update(), _("No file.")
    data = _read_uploaded_file_bytes(file_obj)
    new_entries = import_subtitleedit_multiple_replace_template(data)
    entries = load_glossary(cfg.glossary.json_path) + new_entries
    save_glossary(cfg.glossary.json_path, entries)
    return [[e.en, e.zh, e.note, e.enabled] for e in entries], _("Imported {new_count} from .template (best-effort). Total {total_count}.").format(new_count=len(new_entries), total_count=len(entries))


# -----------------------------
# UI
# -----------------------------
# å¾ language_config.json ç²å–ç¢ºåˆ‡çš„èªè¨€ä»£ç¢¼ç”¨æ–¼æ¨™é¡Œ
_TITLE_LANG = _TARGET_LANG_LOCALE if _TARGET_LANG_LOCALE else "zh-TW"

with gr.Blocks(title=_("Local Subtitle Translator (Vision + Reason)")) as demo:
    gr.Markdown(
        f"# ğŸ¬ Local Subtitle Translator (EN â†’ {_TITLE_LANG})\n"
        f"{_('No Ollama needed. Runs with llama-cpp-python (GGUF) + optional local vision (GGUF vision model).')}\n"
    )

    with gr.Tab(_("Translate")):
        with gr.Row():
            with gr.Column(scale=1):
                # NOTE: Gradio's Video component requires an external ffmpeg executable.
                # To keep this app fully self-contained, we use File input for videos.
                video = gr.File(label=_("Video (MKV/MP4) â€” required for audio and vision analysis"), file_types=[".mp4", ".mkv", ".mov", ".avi"])
                srt = gr.File(label=_("SRT (English)"), file_types=[".srt"])
                srt_encoding = gr.Dropdown(
                    ["utf-8", "utf-8-sig", "cp1252"],
                    value="utf-8",
                    label=_("SRT encoding"),
                )
                # èªè¨€è¨­å®šå·²ç”± language_config.json æ§åˆ¶ï¼Œä¸å†éœ€è¦ UI é¸æ“‡å™¨
                gr.Markdown(
                    f"**{_('Target language')}:** `{_TARGET_LANG_LOCALE}` "
                    f"({_('configured in language_config.json')})"
                )
                # Vision is optional. Leave it off by default so the app works even
                # if you haven't downloaded a vision GGUF + mmproj yet.
                enable_vision = gr.Checkbox(value=False, label=_("Enable vision fallback (Run C/D, local GGUF vision model)"))
                enable_context_expansion = gr.Checkbox(value=True, label=_("Enable context expansion fallback (Run E, need_more_context)"))
                max_frames = gr.Slider(
                    1, 4, value=1, step=1,
                    label=_("Max frames per subtitle (Run D)"),
                )
                offsets = gr.Textbox(
                    value="0.5",
                    label=_("Frame offsets within subtitle span (comma-separated, 0..1)"),
                    placeholder=_("e.g. 0.3,0.7"),
                )
                run_mode = gr.Dropdown(
                    choices=["all", "A", "B", "C", "D", "E", "F"],
                    value="all",
                    label=_("Run mode"),
                    info=_("all = Aâ†’Bâ†’(C/D)â†’Eâ†’F; E = context expansion; F = Translate"),
                )
                run_e_scheme = gr.Dropdown(
                    choices=["full", "main_led", "local_led", "draft_first"],
                    value="full",
                    label=_("Run F scheme (when to use main vs. localization model)"),
                    info=_("Full=both strong | MAIN-led=main strong | LOCAL-led=local strong | Draft-first=both weak. See README Run F."),
                )
                with gr.Row():
                    run = gr.Button(_("ğŸš€ Translate"), variant="primary")
                    btn_reset = gr.Button(_("Reset"), variant="secondary")
                log = gr.Textbox(
                    label=_("Log (last 200 lines)"),
                    lines=14,
                    max_lines=14,
                    interactive=False,
                )

            with gr.Column(scale=1):
                out_file = gr.File(label=_("Download translated SRT"))
                out_names_file = gr.File(label=_("Download names CSV (glossary format)"))
                status = gr.Textbox(label=_("Status (EN)"), value=_("Idle."), interactive=False)
                latest_log = gr.Textbox(
                    label=_("Latest log"),
                    value="",
                    lines=1,
                    max_lines=1,
                    interactive=False,
                )
                img = gr.Image(label=_("Preview frame (when vision runs)"), height=300)

        def reset_translate_tab():
            """ä¸€éµé‡è¨­ï¼šæ¸…ç©ºè¼¸å…¥/è¼¸å‡ºèˆ‡æ—¥èªŒï¼Œæ¢å¾©é è¨­å€¼ï¼Œæ–¹ä¾¿é–‹å§‹æ–°ç¿»è­¯ã€‚"""
            return (
                gr.update(value=None),   # videoï¼ˆFile éœ€ update æ‰æœƒæ¸…ç©ºï¼‰
                gr.update(value=None),   # srt
                "utf-8",  # srt_encoding
                False,  # enable_vision
                True,    # enable_context_expansion
                1,      # max_frames
                "0.5",  # offsets
                "all",  # run_mode
                "full", # run_e_scheme
                gr.update(value=None),   # out_file
                gr.update(value=None),   # out_names_file
                "",     # log
                gr.update(value=None),   # img
                _("Idle. Ready for new translation."),
                "",     # latest_log
                )

        run.click(
            translate_ui,
            inputs=[video, srt, srt_encoding, enable_vision, enable_context_expansion, max_frames, offsets, run_mode, run_e_scheme],
            outputs=[out_file, out_names_file, log, img, status, latest_log],
        )
        btn_reset.click(
            reset_translate_tab,
            inputs=[],
            outputs=[video, srt, srt_encoding, enable_vision, enable_context_expansion, max_frames, offsets, run_mode, run_e_scheme, out_file, out_names_file, log, img, status, latest_log],
        )

    with gr.Tab(_("Glossary")):
        # æ ¹æ“šç›®æ¨™èªè¨€é¡¯ç¤ºå°æ‡‰çš„èªè¨€åç¨±
        target_lang_name = {
            "zh-TW": "ç¹é«”ä¸­æ–‡",
            "zh-CN": "ç°¡é«”ä¸­æ–‡",
            "ja-JP": "æ—¥æœ¬èª",
            "es-ES": "EspaÃ±ol",
        }.get(_TARGET_LANG_LOCALE, _TARGET_LANG_LOCALE)
        
        gr.Markdown(
            _("""### Glossary / Replace Library
- Edit in-table
- Import CSV (2 columns: English, {target_lang})
- Import Subtitle Edit `.template` (best-effort XML extraction)
""").format(target_lang=_TARGET_LANG_LOCALE)
        )
        table = gr.Dataframe(
            headers=[_("English"), target_lang_name, _("Note"), _("Enabled")],
            datatype=["str", "str", "str", "bool"],
            interactive=True,
            row_count=(0, "dynamic"),
            column_count=(4, "fixed"),
        )

        with gr.Row():
            btn_load = gr.Button(_("Load"))
            btn_save = gr.Button(_("Save"))

        with gr.Row():
            csv_file = gr.File(label=_("Import CSV"), file_types=[".csv"])
            tpl_file = gr.File(label=_("Import Subtitle Edit .template"), file_types=[".template"])

        with gr.Row():
            btn_imp_csv = gr.Button(_("Import CSV"))
            btn_imp_tpl = gr.Button(_("Import Subtitle Edit .template"))

        g_status = gr.Textbox(label=_("Glossary status"), interactive=False)

        btn_load.click(glossary_load, inputs=[], outputs=[table, g_status])
        btn_save.click(glossary_save, inputs=[table], outputs=[g_status])
        btn_imp_csv.click(glossary_import_csv, inputs=[csv_file], outputs=[table, g_status])
        btn_imp_tpl.click(glossary_import_template, inputs=[tpl_file], outputs=[table, g_status])

    with gr.Tab(_("Setup / Model Paths")):
        # Ensure config.json exists (do not shadow gettext `_`)
        _ensure_config()
        gr.Markdown(
            _("""### config.json
Edit `config.json` to point to your GGUF model files under `./models/`.

If missing, the Translate tab will show a clear error.
""")
        )

        cfg_box = gr.Code(value=CONFIG_PATH.read_text(encoding="utf-8"), language="json")
        btn_write = gr.Button(_("Write to config.json"))
        cfg_write_status = gr.Textbox(interactive=False)

        def write_cfg(txt: str):
            try:
                CONFIG_PATH.write_text(txt, encoding="utf-8")
                return _("Saved config.json")
            except Exception as e:
                return _("Failed: {error}").format(error=str(e))

        btn_write.click(write_cfg, inputs=[cfg_box], outputs=[cfg_write_status])

if __name__ == "__main__":
    # show_error=True: show backend tracebacks in the browser when something goes wrong.
    demo.launch(inbrowser=True, show_error=True)
